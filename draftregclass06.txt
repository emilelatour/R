Chapter 6  Model Fit: Assessment and Improvement

The Box quote from our first chapter is well worth repeating:

All models are wrong, but some are useful — famed statistician George Box

We have quite a bit of powerful machinery to fit parametric models. But are they any good on a given data set? We’ll discuss this subject here in this chapter.

6.1 Aims of This Chapter

Most regression books have a chapter on diagnostics, methods for assessing model fit and checking assumptions needed for statistical inference (confi- dence intervals and significance tests).

In this chapter we concerned only with the model itself. Does, for instance, the model assume a linear relationship of the response variable with the predictors, when it actually is nonlinear? Are there extreme or erroneous observations that mar the fit of our model?

We are not concerned here with assumptions that only affect inference, as those were treated in Chapter 2 [1]. 

6.2 Methods

There is a plethora of diagnostic methods! Entire books could and have been written on this topic. I treat only a few such methods here — some classical, some of my own — with the choice of methods presented stemming from these considerations:

- This book generally avoids statistical methods that rely on assuming that our sample data is drawn from a normally distributed popula- tion.2 Accordingly, the material here on unusual observations does not make such an assumption.

- Intuitive clarity of a method is paramount. If a method can’t be ex- plained well to say, a reasonably numerate but nonstatistician client, then I prefer to avoid it.

6.3 Notation

Say we have data (X[i], Y[i]), i = 1, ..., n. Here the X[i] are p-component vectors,

    X[i] = (X[i](1), ..., X[i](p))′  (6.1)

and the Y[i] are scalars (including the case Y = 0, 1, ..., m − 1 in classification applications). We typically won’t worry too much in this chapter whether the n observations are independent.

As usual, let

    μ(t) = E(Y | X = t)  (6.2) 

be our population regression function, and let μ^(t) its estimate from our sample data.

6.4 Goals of Model Fit-Checking

What do we really mean when we ask whether a model fits a data set well? Our answer ought to be as follows:

Possible Fit-Checking Goal:

Our model fits well if μ^(t) is near μ(t) for all t, or at least for t = X[1], ..., X[n].

That criterion is of course only conceptual; we don’t know the values of μ(t), so it’s an impossible criterion to verify. Nevertheless, it may serves well as a goal, and our various model-checking methods will be aimed at that goal.

Part of the answer to our goals question goes back to the twin regression goals of Prediction and Description. We’ll explore this in the following sections.

6.4.1 Prediction Context

If our regression goal is Prediction and we are doing classification, our above Fit-Checking Goal may be much too stringent. Say for example m = 2. If μ(t) is 0.9 but μ&(t = 0.62), we will still make the correct guess, Y = 1, even though our regression function estimate is well off the mark. Similarly, if μ(t) is near 0 (or less than 0.5, actually), we will make the proper guess for Y as long as our estimated value μ&(t) is under 0.5.

Still, other than the classification case, the above Fit-Checking Goal is appropriate. Errors in our estimate of the population regression function will impact our ability to predict, no matter what the size is of the true regression function.

6.4.2 Description Context

Good model fit is especially important when our regression goal is Description. We really want assurance that the estimated regression coefficients represent the true regression function well. since we will be using them to describe the underlying process.

6.4.3 Center vs. Fringes of the Data Set

Consider a Prediction setting, in the classification case. In Section 6.4.1 above, we saw that actually can afford a rather poor model fit in regions of the predictor space in which the population regression function is near 1 or 0.

The same reasoning shows, though, that having a good fit in regions where μ(t) is mid-range, say in (0.25,0.75) is important. If μ^(t) and μ(t) are on opposite sites of the number 0.5 (i.e. one below 0.5 and the other above), we will make the wrong decision (even though we make still be lucky and guess Y correctly).

In classification contexts, the p-variate density of X is often “mound-shaped,” if not bell-shaped, within each class. (In fact, many clustering algorithms are aimed at this situation.) For such data, the regions of most sensitivity in the above sense will be the lines/curves separating the pairs of mounds. (Recall Figure 5.1.) The fringes of the data set, far from these pairwise boundaries, will be regions in which model fit is less important, again assuming a Prediction goal.

In regression contexts (continous Y , count data etc.), the full data set will tend to be mound-shaped. Here good estimation will be important for Predicition and Description throughout the entire region. However, one must keep in mind that model fit will typically be better near the center of the data than at the fringes — and that the observations at the fringes typically have the heaviest impact on the estimated coefficients. This latter consideration is of course of great import in the Description case.

We will return to these considerations at various points in this chapter.

6.5 Example: Currency Data

Fong and Ouliaris (1995) do an analysis of relations between currency rates for Canada, Germany, France, the UK and Japan (pre-European Union days). Do they move together? Let’s look at predicting the Japanese it yen from the others.

This is time series data, and the authors of the above paper do a very sophisticated analysis along those lines. So, the data points, such for the pound, are not independent through time. But since we are just using the data as an example and won’t be doing inference (confidence intervals and significance tests), we will not worry about that here.

Let’s start with a straightforward linear model:

> curr <− read.table(’EXC.ASC’, header = TRUE) 
> fout <− lm(Yen ~ ., data = cur1)
> head(curr)
> summary( fout )

Not surprisingly, this model works well, with an adjusted R-squared value of about 0.89. The signs of the coefficients are interesting, with the it yen seeming to fluctuate opposite to all of the other currencies except for the German mark. Of course, professional financial analysts (domain experts, in the data science vernacular) should be consulted as to the reasons for such relations, but here we will proceed without such knowledge.

It may be helpful to scale our data so as to better understand the roles of the predictors, though, so as to put make all the predictors commensurate. Each predictor will be divided by its standard deviation (and have its mean subtracted off), so all the predictors have standard deviation 1.0:
value

> curr1 <− curr
> curr1[,−5] <− scale(curr1[ , −5]) 
> fout1 <− lm(Yen ~ .,data = curr1) 
> summary( fout1 )

So Germany and France appear to have the most influence [3]. This, and the signs (positive for the mark, negative for the franc), form a good example of the use of regression analysis for the Description goal.

In the next few sections, we’ll use this example to illustrate the basic conceptss.

6.6 Overall Measures of Model Fit

We’ll look two broad categories of fit assessment methods. The first will consist of overall measures, while the second will involve relating fit to individual predictor variables.

6.6.1 R-Squared, Revisited

We have already seen one overall measure of model fit, the R-squared value (Section ??). As noted before, its cousin, Adjusted R-squared, is considered more useful, as it is aimed at compensating for overfitting.

For the currency data above, the two R-squared values (ordinary and ad- justed) were 0.8923 and 0.8918, both rather high. Note that they didn’t differ much from each other, as there were well over 700 observations, which should easily handle a model with only 4 predictors (a topic we’ll discuss in Chapter 9).

Recall that R-squared, whether a population value or the sample estimate reported by lm(), is the squared correlation between Y and its predicted value μ(X) or μ^(X), respectively. Thus it can be calculated for any method of regression function estimation, not just the linear model. In particular, we can apply the concept to k-Nearest Neighbor methods.

The point of doing this with kNN is that the latter in principle does not have model-fit issues. Whereas our linear model for the currency data assumes a linear relationship between the Yen and the other currencies, kNN makes no assumptions on the form of the regression function. If kNN were to have a substantially larger R-squared value than that of our linear model, then we may be “leaving money on the table,” i.e. not fitting as well as we could with a more sophisticated parametric model [4].

So, let’s see what kNN tells us in the currency example:

> xdata <− preprocessx (curr1[ , −5], 25, xval = TRUE) 
> kout <− knnest(curr1[ , 5], xdata, 25)
> cor(kout$regest, curr1[ , 5])ˆ2
[1] 0.9717136

This would indicate that, in spite of a seemingly good fit for our linear model, it does not adequately describe the currency fluctuation process.

However, we should raise a question as to whether the value of k here, 25, is a good one. Let’s investigate this with the regtools function kmin():

> xdata <− preprocessx(curr1[ , −5], 150, xval = TRUE)
> kminout <− kmin(curr1$Yen, xdata, predwrong, nk = 30) 
> kminout$kmin
[1] 5

The “best” k, as determined by a cross-validated estimate of mean squared prediction error, is reported here to be 5 (which is the smallest value this function tried). This may seem very small, but as will be discused in Chapter 10, it is to be expected in light of the high R^2 value we have with this data.

Let’s rerun our R^2 calculation with this value of k:

> kout <− knnest(curr1[ , 5], xdata, 5) 
> cor(kout$regest, curr1[ , 5])ˆ2
[1] 0.9920137

Even better! So, our linear model, which seemed so nice at first, is missing something. Maybe we can determine why via the methods in the following sections.

6.6.2 Plotting Parametric Fit Against Nonparametric One

Let’s plot the μ^ values of the linear model against those of kNN: 

> parvsnonparplot(fout1, kout)

The result is shown in Figure 6.1. It suggests that the linear model is overestimating the regression function at times when the Yen is very low, moderately low or very high, and possibly underestimating in the moderately high range.

We must view this cautiously, though. First, of course, there is the issue of sampling variation; the apparent model bias effects here may just be sampling anomalies.

Second, kNN itself is subject to some bias at the edges of a data set. This will be discussed in detail in Section ??, but the implications are that in the currency case kNN tends to overestimate for low values of the Yen, and underestimate at the high end. This can be addressed by doing locally- linear smoothing, an option offered by knnest(), but let’s not use it for now.

The “hook shape” at the left end, and a “tail” in the middle suggest odd nonlinear effects, possibly some local nonlinearities, which kNN is picking up but which the linear model misses.

6.6.3 Residuals vs. Smoothing

In any regression analysis, the quantities

    r[i] = Y[i] − μ^(X[i])  (6.3)

are traditionally called the residual values, or simply the residuals. They are of course the prediction errors we obtain when fitting our model and then predicting our Yi from our Xi. The smaller these values are in absolute value, the better, but also we hope that they may inform us of inaccuracies in our model, say nonlinear relations between Y and our predictor variables.

In the case of a linear model, the residuals are

    r[i] = Y[i] − β0^ − β1^*X[i]^(1) − ... −βp^*X[i]^(p)  (6.4)

Figure 6.1: Estimation of Regression Values, Two Methods

Many diagnostic methods for checking linear regression models are based on residuals. In turn, their convenient computation typically involves first computing the hat matrix, about which there is some material in the Mathematical Complements section at the end of this chapter.

The generic R function plot() can be applied to any object of class ”lm” (including the subclass ”glm”). Let’s do that with fout1:

> plot(fout1)
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
Hit <Return> to see next plot:

We obtain a series of graphs, displayed sequentially. Most of them involve more intricate concepts than we’ll use in this book (recall Section 6.2), but let’s look at the first plot, shown in Figure 6.2. The “hook” and “tail” are visible here too.
Arguably the effects are more clearly in Figure 6.1. This is due to the fact that the latter figure is plotting smoothed values, not residuals. In other words, residuals and smoothing play complementary roles to each other: Smoothing-based plots can more easily give us “the big picture,” but residuals may enable us to spot some fine details.

In any case, it’s clear that the linear model does not tell the whole story.

6.7 Diagnostics Related to Individual Predictors

It may be that the relationship with the response variable Y is close to linear for some predictors X^(i) but not for others. How might we investigate that?

6.7.1 Partial Residual Plots

We might approach this by simply plotting a scatter diagram of Y against each predictor variable. However, the relation of Y with X^(i) may change in the presence of the other X^(j). A more sophisticated approach may be partial residual plots, also known as component + residual plots. These would be easy to code on one’s own, but the crPlot() function in the car package does the job nicely for us:

Figure 6.2: Residuas Against Linear Fitted Values

Figure 6.3: Partial Residuals Plot, Currency Data

> crPlots(fout1)

The resulting graph is shown in Figure 6.3. Before discussing these rather bizarre results, let’s ask what these plots are depicting.

Here is how the partial-residual method works. The partial residuals for a predictor X^(j) are defined to be

p[i] = r[i] +β[j]*X[i]^(j)   (6.5)
     = Y[i] − β0 − β1*X[i]^(1) − ... −β1*X[i]^(j − 1) − β1*X[i]^(j + 1) − ... − βp*X[i]^(p) (6.6)

In other words, we’ve started with (6.4), but removed the linear term contributed by predictor j, i.e. removed βp*X[i]^(j). We then plot the p[i] against i predictor j, to try to discern a relation. In effect we are saying,

Cancel that linear contribution of predictor j. Let’s start fresh with this predictor, and see how adding it in a possibly nonlinear form might extend the collective predictive ability of the other predictors.

If the resulting graph looks nonlinear, we may profit from modifying our model to one that reflects a nonlinear relation.

In that light, what might we glean from Figure 6.3? First, we see that the only “clean” relations are the one for the franc and the one for the mark. No wonder, then, that we found earlier that these two currencies seemed to have the strongest linear relation to the Yen. There does seem to be some nonlinearity in the case of the franc, with a more negative slope for low franc values, and this may be worth pursuing, say by adding a quadratic term.

For the Canadian dollar and the pound, though, the relations don’t look “clean” at all. On the contrary, the points in the graphs clump together much more than we typically encounter in scatter plots.

But even the mark is not off the hook (pardon the pun), as the “hook” shape noticed earlier is here for that currency, and apparently for the Canadian dollar as well. So, whatever odd phenomenon is at work may be related to these two currencies,

6.7.2 Plotting Nonparametric Fit Against Each Predictor

As noted, one approach would be to draw many scatter diagrams, plotting Y individually against each X(i). But scatter diagrams are, well, scattered. A better way is to plot the nonparametric fit against each predictor. The regtools function nonparvsxplot() does this, plotting one graph for each predictor, presented in succession with user prompts:

> nonparvsxplot(kout) 
next plot
next plot
next plot
next plot

Figure 6.4: Nonparametric Fit Against the Mark

The graph for for the mark is shown in Figure 6.4. Oh my gosh! With the partial residual plots, the mark and the franc seemed to be the only “clean” ones. Now we see that the situation for the mark is much more complex. The same is true for the other predictors (not shown here). This is indeed a difficult data set.

Again, note that the use of smoothing has brought these effects into better focus, as discussed in Section 6.6.3.

6.7.3 Freqparcoord

Another graphical approach is via freqparcoord package, written by Yingkang Xie and me. The call produces the graph in Figure 6.5.

Figure 6.5: Freqparcoord Plot, Currency Data

What is depicted in this graph? First, this is a parallel coordinates plot, which is a method for visualizing multidimensional data in a 2-dimensional graph. This approach dates back to the 1800s, but was first developed in depth in modern times; see Inselberg (2009).

The general method of parallel coordinates is quite simple. Here we draw p vertical axes, one for each variable. For each of our n data points, we draw a polygonal line from each axis to the next. The height of the line on axis j is the value of variable j for this data point. As Inselberg pointed out, in mathematical terms the plot performs a transformation mapping p-dimensional points to (p − 1)-segment lines. The practical effect is that we can visualize how our p variables vary together.

However, if our number of data points n is large, our parallel coordinates will consist of a chaotic jumble of lines, maybe even with the “black screen problem,” meaning that so much has been plotted that the graph is mostly black, no defining features.

The solution to that problem taken by freqparcoord is to plot only the most frequently-occurring lines, meaning those corresponding to the original data points having the largest estimated p-dimensional density funciton.

A function in the freqparcoord package, regdiag(), applies this to regres- sion diagnostics. The first variable plotted, i.e. the first vertical axis, is what we call the divergences, meaning the differences beween the parametric and nonparametric estimates of the population regression function,

μ[linmod](X[i]) − μ[knn](X[i]), i = 1, ..., n  (6.7) 

The other axes represent our predictor variables. Vertical measures are numbers of standard deviations from the mean of the given variable.

There are three groups, thus three subgraphs, for the upper 10%, middle 80% and lower 10% of the divergence values. So for instance the upper subgraph describes data points Xi at which the linear model greatly overestimates the true regression function.

What we see, then, is that in regions in which the linear model underestimates, the Canadian dollar tends to be high and the mark low, with an opposite relation for the region of overestimation. Note that this is not the same as saying that the correlation between those two currencies is negative; on the contrary, running cor(curr1) shows their correlation to be positive and tiny, about 0.01. This suggests that we might try adding a dollar/mark interaction term to our model, though the effect here seems mild, with peaks and valleys of only about 1 standard deviation.

6.8 Effects of Unusual Observations on Model Fit

Suppose we are doing a study of rural consumer behavior in a small country C in the developing world. One day, a few billionaires discover the idyllic beauty of rural C and decide to move there. We almost certainly would want to exclude data on these interlopers from our analysis. Second, most data contains errors. Obviously these must be excluded too, or corrected if possible. These two types of observations are sometimes collectively called outliers, or simply unusual.

Though we may hear that Bill Gates has moved to C, and we can clean the data to remove the obvious errors, such as a human height of 25 feet or a negative weight. other extreme values or errors may not jump out at us. Thus it would be useful to have methods that attempt to find such observations in some mechanical way.

6.8.1 The influence() Function

Base R includes a very handy function, influence(). We input an object of type ”lm”, and it returns an R list. One of the components of that list, coefficients, is just what we want: It has a column for each βj, and a row for each observation in our data set. Row i, column j tells us how much β would change if observation i were deleted from the data set [5]. If the j change is large, we should take a close look at observation i, to determine whether it is “unusual.”

6.8.1.1 Example: Currency Data

Let’s take a look:

> infcfs <− influence(fout1)$coef 
> head(infcfs)

￼5The entire computation does not need to be done from scratch. The Sherman- Morrison-Woodbury formula provides a shortcut. See the Mathematical Complements section at the end of this chapter.

So, if we were to remove the second data point, this says that β0 would 􏰭
decline by 0.02018040, β1 would increase by 0.03743135, and so on. Let’s check to be sure:
> coef(fout1) ( Intercept ) 224.945099 Pound −5.331583
Can Mark −5.615144 57.888556
Franc −34.702731
Franc −34.607843
Excellent. Now let’s find which points have large influence.
A change in an estimated coefficient should be considered “large” only rel- ative to the standard error, so let’s scale accordingly, dividing each change by the standard error of the correspoding coefficient:
> se <− sqrt(diag(vcov(fout1)))
> infcfs <− infcfs %∗% diag(1/se)
So, how big do the changes brought by deletions get in this data? And for which observations does this occur?
> ia <− abs(infcfs) > max(ia)
[1] 0.1928661
> f15 <− function(rw) any(rw > 0.15) > i a 1 5 <− a p p l y ( i a , 1 , f 1 5 )
> names(ia15) <− NULL
> which(ia15)
[1] 744 745 747 748 749 750 751 752 753 754 755 [12] 756 757 758 759 760 761
Here we (somewhat arbitrarily) decided to identify which deletions of ob- servations would result in an absolute change of some coefficient of more than 0.15.
Now this is interesting. There are 761 observations in this data set, and now we find that all of the final 18 (and more) are influential. Let’s look more closely:
> coef(lm(Yen  ̃ .,data=curr1[−2,]))
( Intercept ) 224.965279 Pound −5.432882 > −5.652575
Can Mark −5.652575 57.906698
+ 0.037431 [ 1 ] −5.615144

> tail(ia,5)
[,1] [,2] [,3] [,4]
757 0.05585538 0.1311110 0.1572411 0.1305925 758 0.05851087 0.1294412 0.1563013 0.1259741 759 0.05838813 0.1386614 0.1629851 0.1358875 760 0.05818730 0.1429951 0.1654354 0.1385146 761 0.05626212 0.1316884 0.1534207 0.1211305
[ ,5] 757 0.021300177 758 0.015701005 759 0.020431391 760 0.019962502 761 0.006793673
So, the influence of these final observations was on the coefficients of the Canadian dollar, the mark and the franc — but not on the one for the pound.
Something special was happening in those last time periods. It would be imperative for us to track this down with currency experts.
Each of the observations has something like a 0.15 impact, and intuitively, removing all of these observations should cause quite a change. Let’s see:
> curr2 <− curr1 [ −(744:761) ,] > lm(Yen  ̃ .,data=curr2)
...
Coefficients :
( Intercept ) Canada Mark
225.780 −10.271 52.926
Franc −27.126
> fout1 ...
Coefficients : (Intercept) 224.945 Franc −34.703
Pound −6.431
Canada Mark −5.615 57.889
Pound −5.332
These are very
rency almost doubled, and even the pound’s value changed almost 30%. That latter is a dramatic difference, in view of the fact that each individual
substantial changes! The coefficient for the Canadian cur-

observation had only about a 2% influence on the pound.
6.9 Automated Outlier Resistance
The term robust in statistics generally means the ability of methodology to withstand problematic conditions. Linear regression models, for instance, are said to be “robust to the normality” assumption, meaning that (at least large-sample) inference on the coefficients with work well even though the distribution of Y given X is not normal.
Here we are concerned with robustness of regression models to outliers. Such methods are called robust regression. There are many such methods, one of which is median regression, to be discussed next.
6.9.1 Median Regression
Suppose we wish to estimate the mean of some variable in some population, but we are concerned about unusual observations. As an alternative, we might consider estimating the median value of the variable, which will be much less sensitive to unusual observations. Recall our hypothetical exam- ple earlier, in which we were interested in income distributions. If one very rich person moves into the area, the mean may be affected substantially — but the median would likely not change at all.
We thus say that the median is robust to unusual data points. One can do the same thing to make regression analysis robust in this sense.
It turns out (see the Mathematical Complements section at the end of this chapter) that
ν(t) = median(Y | X = t) = argminmE(|Y − m| |X = t) (6.8)
In other words, in contrast to the regression function, i.e. the conditional mean, which minimizes mean squared prediction error, the conditional me- dian minimizes mean absolute error.
Remember, as with regression, we are estimating an entire function here, as t varies. A nonparametric approach to this would be to use knnest() with nearf set to
function ( predpt , nearxy )
{
ycol <− ncol(nearxy)
median(nearxy[, ycol]) }
However, in this chapter we are primarily concerned with parametric mod- els. So, we might, in analogy to the linear regression model, make the assumption that (6.8) has the familiar form
ν(t)=β0 +β1t1 +...+βptp (6.9)
Solving this at the sample level is a linear programming problem, which has been implemented in the CRAN package quantreg. As the package name implies, we can estimate general conditional quantile functions, not just the conditional median. The argument tau of the rq() function specifies what quantile we want, with the value 0.5, i.e. the median, being the default.
It is important to understand that ν(t) is not the regression function, i.e. not the conditional mean. Thus rq() is not estimating the same quantity as is lm(). Thus the term quantile regression, in this case the term me- dian regression, is somewhat misleading here. But we can use ν(t) as an alternative to μ(t) in one of two senses:
(a) We may believe that ν(t) is close to μ(t). They will be exactly the same, of course, if the conditional distribution of Y given X is sym- metric, at least if the unusual observations are excluded. (This is an assumption we can assess by looking at the residuals.)
(b) We may take the point of view that the conditional median is just as meaningful as the conditional mean (no pun intended this time), so why not simply model ν(t) in the first place?
Sense (a) above will be particularly relevant here.
6.9.2 Example: Currency Data
Let’s apply rq() to the currency data:
> qout <− rq(Yen  ̃ .,data=curr1) > qout
... Coefficients :
( Intercept ) 224.517899 Pound −5.320035
...
> fout1
... Coefficients : ( Intercept )
224.945 Franc −34.703

Can −11.038238
Mark Franc 53.854005 −27.443584
Can Mark −5.615 57.889
Pound −5.332
The results are
calling lm() with the bad observations at the end of the data set removed. In other words,
Median regression can be viewed as an automated method for removing (the effects of) the unusual data points.
6.10 Example: Vocabulary Acquisition
The Wordbank data, http://wordbank.stanford.edu/, concerns child vo- cabulary development, in not only English but also a number of other lan- guages, such as Cantonese and Turkish. These are mainly toddlers, ages from about a year to 2.5 years.
Let’s read in the English set:
> engl <− read.csv(’English.csv’)
> engl <− engl [ ,c(2 ,5:8 ,10)]
> encc <− engl[complete.cases(engl),] > nrow(encc)
[1] 2741
One of the variables is Birth Order. Let’s make it numeric:
> z <− engcc$birth order
> numorder <− vector(length=length(z))
> for (i in 1:length(z)) {
+ numorder[i] <− if(z[i]==’First’) 1 else + if(z[i]==’Second’) 2 else
strikingly similar to what we obtained in Section 6.8.1.1 by

+ if(z[i]==’Third’) 3 else
+ if(z[i]==’Fourth’) 4 else
+ if(z[i]==’Fifth’) 5 else
+ if(z[i]==’Sixth’) 6 else
+ if(z[i]==’Seventh’) 7 else 8 +}
> encc$birthord <− numorder
Let’s convert the variable on the mother’s education to a rough number of years of school:
> z <− encc$mom ed
> momyrs <− vector(length=length(z))
> for (i in 1:length(z)) {
￼momyrs[i] <−
if(z[i]==’Primary’) 4 else if(z[i]==’Some Secondary’) 10 else if(z[i]==’Secondary’) 12 else if(z[i]==’Some College’) 14 else if(z[i]==’College’) 16 else if(z[i]==’Some Gradute’) 18 else 20
+
+
+
+
+
+
+}
> encc$momyrs <− momyrs
Also, create the needed dummy variables, for gender and nonwhite cate- gories:
> encc$male <− as.numeric(encc$sex==’Male’)
> encc$asian <− as.numeric(encc$ethnicity==’Asian’)
> encc$black <− as.numeric(encc$ethnicity==’Black’)
> encc$latino <− as.numeric(encc$ethnicity==’Hispanic ’)
> encc$othernonwhite <− as.numeric(encc$ethnicity==’Other ’)
Running knnest() (not shown), there seemed to be an approximately linear relation between vocabulary and age (for the age range studied). Let’s run a linear regression analysis:
> encc1 <− encc[,−c(2:5)]
> summary(lm(vocab  ̃ .,data=encc1))
... Coefficients :
( Intercept ) age birthord
Estimate −473.0402 33.3905 −20.5399
Std . Error t 23.1001 −20.478
Pr(>|t |)
< 2e−16 ∗∗∗
value
0.6422 51.997 < 2e−16 ∗∗∗ 3.0885 −6.650 3.52e−11 ∗∗∗

male
asian
black
othernonwhite
latino −75.6905
momyrs 3.6746 ...
−49.0196 −15.7985 1.0940 −54.1384
5.4840 17.2940 10.2360 15.1274 13.0155
−8.939 < 2e−16 ∗∗∗ −0.914 0.361048
0.107 0.914890 −3.579 0.000351 ∗∗∗ −5.815 6.75e−09 ∗∗∗
3.917 9.18e−05 ∗∗∗ 0.5118
0.9381
Multiple R−squared: 0.5132, Adjusted R−squared:
So, the kids seemed to be learning about 30 characters per month during the studied age range. Latino kids seem to start from a lower English base, possibly due to speaking Spanish at home. Consistent with the general notion that girls develop faster than boys, the latter have a lower base. Having a lot of older siblings also seems to be related to a lower base, possibly due to the child being one of several competing for the parents’ attention. Having a mother with more education had a modest positive effect.
A word on the standard errors: As each child was measured multiple times as he/she aged, the observations are not independent, and the true standard errors are larger than those given.
Let’s try median regression instead:
> rq(vocab  ̃ .,data=encc1) ...
Coefficients : ( Intercept )
asian −574.685185
−21.944444 black
−13.148148
The robust results here are similar to what we obtained earlier, but with
some modest shifts.
It is often worthwhile to investigate other quantiles than the median. Trying that for age only:
> plot(c(12,30),c(0,800),type = ”n”, xlab = ”age”, ylab > abline(coef(rq(vocab  ̃ age,data=encc)))
> abline(coef(rq(vocab  ̃ age,data=encc,tau=0.9)))
> abline(coef(rq(vocab  ̃ age,data=encc,tau=0.1)))
...
age
37.777778
birthord −19.425926
male −44.925926
othernonwhite −50.462963
latino −68.629630
momyrs 3.638889
= ”vocab”)

Figure 6.6: Vocabulary vs. Age
As seen in Figure 6.6, the middle-level children start out knowin much fewer words than the most voluble ones, but narrow the gap over time. By contrast, the kids with smaller vocabularies start out around the same level as the middle kids, but actually lose ground over time, suggesting that educational interventions may be helpful.
6.11 Improving Fit
The currency example seemed so simple at first, with a very nice adjusted R-squared value of 0.89, and with the yen seeming to have a clean linear relation with the franc and the mark. And yet we later encountered some troubling aspects to this data.
First we noticed that the adjusted R-squared value for the kNN fit was even better, at 0.98. Thus there is more to this data than simple linear

relationships. Later we found that the last 18 data points, possibly more,
have an inordinate influence on the βj. This too could be a reflection of nonlinear relationships between the currencies. The plots exhibited some strange, even grotesque, relations.
So, let’s see what we might do to improve our parametric model. 6.11.1 Deleting Terms from the Model
Predictors with very little relation to the response variable may actually degrade the fit, and we should consider deleting them. Tnis topic is treated in depth in Chapter 9.
6.11.2 Adding Polynomial Terms
Our current model is linear in the variables. We might add second-degree terms. Note that this means not only squares of the variables, but products of pairs of them. The latter may be important, in view of our comment in Section 6.7.3 that it might be useful to add a Canadian dollar/mark interaction term to our model.
6.11.2.1 Example: Currency Data
Let’s add squared terms for each variable, and try the interaction term as well. Here’s what we get:
> curr2 <− curr1
> curr2$C2 <−
> curr2$M2 <−
> curr2$F2 <−
> curr2$P2 <−
> curr2$CM <−
> summary(lm(Yen  ̃ .,data=curr2))
... Coefficients :
( Intercept ) Can
Mark
Franc
Pound
Estimate 223.575386 −8.111223 50.730731 −34.082155 −3.100987
Std . Error t value 1.270220 176.013 1.540291 −5.266 1.804143 28.119 2.543639 −13.399 1.699289 −1.825
curr2$Canadaˆ2 curr2$Markˆ2 curr2$Francˆ2 curr2$Poundˆ2 curr2$Canada∗ curr2$Mark
􏰭
￼6.11.
IMPROVING FIT
−1.514778 −7.113813 11.182524 −1.182451
177
C2
M2
F2
P2
CM 0.003089
0.848240 1.175161 1.734476 0.977692 1.432842
−1.786 −6.053 6.447 −1.209 0.002
( Intercept ) Can
Mark
Franc
Pound C2
M2
F2
P2 CM −−−
Pr(>|t |)
< 2e−16 ∗∗∗
1.82e−07 ∗∗∗ < 2e−16 ∗∗∗ < 2e−16 ∗∗∗
0.0684 .
0.0745 . 2.24e−09 ∗∗∗ 2.04e−10 ∗∗∗
0.2269 0.9983
Signif. codes: 0 ∗∗∗ ...
Multiple R−squared: 0.9043,
∗∗ 0.01 ∗ 0.05 . 0.1
Adjusted R-squared increased only slightly. And this was despite the fact that two of the squared-variable terms were “highly significant,” adorned with three asterisks, showing how misleading significance testing can be. The interaction term came out tiny, 0.003089. So, kNN is still the winner here.
6.11.2.2 Example: Programmer/Engineer Census Data
Let’s take another look at the Census data on programmers and engineers in Silicon Valley, first introduced in Section 1.11.1.
We run
> data(prgeng)
> pe <− prgeng # see ?knnest
> # dummies for MS, PhD
> pe$ms <− as.integer(pe$educ == 14)
> pe$phd <− as . integer ( pe$educ == 16)
> # computer occupations only
> pecs <− pe[pe$occ >= 100 & pe$occ <= 109,] > pecs1 <− pecs [ ,c(1 ,7 ,9 ,12 ,13 ,8)]
> # predict wage income from age , gender etc .
0.001
Adjusted R−squared: 0.9032
￼178CHAPTER 6. MODEL FIT: ASSESSMENT AND IMPROVEMENT
Figure 6.7: Mean Wage Income vs. Age
> # prepare nearest−neighbor data
> xdata <− preprocessx(pecs1[,1:5],150) > kmin(pecs1[,6],xdata,nk=30)$kmin
[1] 5
> zout <− knnest(pecs1[,6],xdata,5) > nonparvsxplot(zout)
we find that the age variable, and possibly wkswrkd, seem to have a quadratic relation to wageinc, as seen in Figures 6.7 and 6.8. So, let’s try adding quadratic terms for those two variables. And, to assess how well this works, let’s break the data into training and test sets:
> pecs2 <− pecs1
> pecs2$age2 <− pecs1$ageˆ2
> pecs2$wks2 <− pecs1$wkswrkdˆ2 > n <− nrow(pecs1)
> trnidxs <− sample(1:n,12000)

Figure 6.8: Mean Wage Income vs. Weeks Worked

> predidxs <− setdiff(1:n,trnidxs)
> lmout1 <− lm(wageinc  ̃ .,data=pecs1[trnidxs ,]) > lmout2 <− lm(wageinc  ̃ .,data=pecs2[trnidxs ,]) > lmpred1 <− predict ( lmout1 , pecs1 [ predidxs , ] )
> lmpred2 <− predict ( lmout2 , pecs2 [ predidxs , ] )
> ypred <− pecs1$wageinc [ predidxs ]
> mean(abs(ypred−lmpred1))
[1] 25721.5
> mean(abs(ypred−lmpred2))
[1] 25381.08
So, adding the quadratic terms helped slightly, about a 1.3% improvement. From a Prediction point of view, this is at best mild, There was also a slight increase in adjusted R-squared, from 0.22 (not shown) to 0.23 (shown below).
But for Description things are
> summary(lmout2) ...
Coefficients :
Estimate ( Intercept ) −63812.415 age 3795.057 sex −10336.835 wkswrkd 598.969 ms 14810.929 phd 20557.235 age2 −39.833 wks2 9.874
Pr(>|t |)
much more useful here:
(Intercept) <
2e−16 ∗∗∗ 2e−16 ∗∗∗ 2e−16 ∗∗∗
age
sex wkswrkd
<
<
5.29e−06 ∗∗∗ < 2e−16 ∗∗∗ < 2e−16 ∗∗∗ < 2e−16 ∗∗∗
8.20e−06 ∗∗∗ Multiple R−squared: 0.2385,
ms phd age2 wks2
...
Std . Error 4471.602 221.615 841.067 131.499 928.536 2197.921 2.608 2.213
t value −14.271 17.125 −12.290 4.555 15.951 9.353 −15.271 4.462
As usual, we should not make too much of the p-values, especially with a sample size this large (16411 for pecs1). So, all those asterisks don’t tell
Adjusted R−squared:
0.2381

us too much. But a confidence interval computed from the standard error shows that the absolute age-squared effect is at least about 34, far from 0, and it does make a difference, say on the first person in the sample:
> predict(lmout1 , pecs1 [1 ,]) 1
62406.48
> predict(lmout2 , pecs2 [1 ,])
1 63471.53
The more sophisticated model predicts about an extra $1,000 in wages for this person.
Most important, the negative sign for the age-squared coefficient shows that income tends to level off and even decline with age, something that could be quite interesting in a Description-based analysis.
The positive sign for wkswrkd is likely due to the fact that full-time work- ers tend to have better jobs.
6.12 Classification Settings
Since we treat classification as a special case of regression, we can use the same fit assessment methods, though in some cases some adapting of them is desirable.
6.12.1 Example: Pima Diabetes Study
Let’s illustrate with the Pima diabetes data set from Section 4.4.2.
> pima <− read.csv(’../Data/Pima/pima−indians−diabetes.data’)
It goes without saying that with any data set, we should first do proper cleaning.6 This data is actually a very good example. Let’s first try the freqparcoord package:
> library(freqparcoord)
> freqparcoord(pima[,−9],−10)
6And of course should have done so for the other data earlier in this chapter, but I decided to keep the first analyses simple.

Here we display that 10 data points (predictors only, not response variable) whose estimated joint density is lowest, thus qualifying as “unusual.”
The graph is shown in Figure 6.9. Again we see a jumble of lines, but look at the big dips in the variables BP and BMI, blood pressure and Body Mass Index. They seem unusual. Let’s look more closely at blood pressure:
> table(pima$BP)
0 24 30 35 1 2 56 58 60 122137 75 76 78
38 40 44 1 1 4 61 62 64 1 34 43 80 82 84 403023 100 102 104
46 48 50 52 2 5 13 11 65 66 68 70 7304557 85 86 88 90 6212522
54 55 11 2 72 74 44 52 92 94
83945 95 96 98
8 6 106 108 110 114 122 14331232311
No one has a blood pressure of 0, yet 35 women in our data set are reported as such. The value 24 is suspect too, but the 0s are wrong for sure. What about BMI?
> table(pima$BMI)
0 18.2 18.4 19.1 19.3 19.4 19.5 19.6 19.9 20 11 3 1 1 1 1 2 3 1 1 20.1 20.4 20.8 21 21.1 21.2 21.7 21.8 21.9 22.1
...
Here again, the 0s are clearly wrong. So, at the very least, let’s exclude such data points:
> p i m a <− p i m a [ p i m a $ B P > 0 & p i m a $ B M I > 0 , ] > dim(pima)
[1] 729 9
(We lost 38 cases.)
Now, for our analysis, start with fitting a logit model, then comparing to
kNN. First, what value of k should we use?:
> glmout <− glm( Diab  ̃ . , data=pima , family=binomial )
> xdata <− preprocessx (pima[ , −9] ,150 , xval=TRUE)
> kminout <− kmin(pima$Diab,xdata,lossftn=predwrong,nk=30) > kminout$kmin
[1] 65

Figure 6.9: Outlier Hunt

Figure 6.10: Best k for Pima

Note that here we used the predwrong() loss function, which computes the misclassification rate.
The “best” value of k found was 65, but the plot suggests that smaller values might be tried, as seen in Figure 6.10. Let’s go with 50, and compare the parametric and nonparametric fits:
> kout <− knnest(pima$Diab,xdata,50) > parvsnonparplot(glmout,kout)
The results of the plot are shown in Figure 6.11. There does appear to be some overestimation by the logit at very high values of the regression function, indeed all the range past 0.5. This can’t be explained by the fact, noted before, that kNN tends to underestimate at the high end.
Note carefully that if our goal is Prediction, it may not matter much at the high end. Recall the discussion on classification contexts in Section 6.4.1. If the true population regression value is 0.8 and we estimate it to be 0.88, we

still predict Y = 1, which is the same as what we would predict if we knew the true population regression function. Similarly, if the true regression value is 0.25 but we estimate it to be 0.28, we still make the proper guess for Y .
For Description, though, we should consider a richer model. Running non- parvsxplot() (not shown) suggests adding quadratic terms for the vari- ables Gluc, Insul, Genet and especially Age. Adding these, and rerun- ning knnest() with nearf = loclin to deal with kNN’s high-end bias, our new parametric-vs.-nonparametric plot is shown in Figure 6.12.
The reader may ask if now we have underestimation by the parametric model at the high end, but we must take into account the fact that with nearf = loclin, we can get nonparametric estimates for the regression function that are smaller than 0 or greater than 1, which is impossible in the classification setting. The perceived “underestimation” actually occurs at values at which the nonparametric figures are larger than 1.
In other words, we now seem to have a pretty good model.
6.13 Special Note on the Description Goal
If we are unable to improve the fit of our parametric model in a setting in which kNN seems to give a substantially better fit, we should be quite wary
of placing too much emphasis on the values of the βj. As we saw with the currency data, the estimated coefficients can be quite sensitive to unusual observations and so on.
This is not to say the βj are useless in such settings. On the contrary, they may be quite valuable. But they should be used with caution.
6.14 Mathematical Complements 6.14.1 The Hat Matrix
We’ll use the notation of Section 2.3.2 here. The hat matrix is defined as
H = A(A′A)−1A′ (6.10) The name stems from the fact that we use H to obtain “Y-hat,” the pre-
􏰭
􏰭

Figure 6.11: Estimation of Regression Values, Two Methods

￼Figure 6.12: Estimation of Regression Values, Two Methods

dicted values for the elements of D: 􏰭􏰰􏰭
so
D􏰭 = Aβ􏰭 = A(A′A)−1A′D = HD
Using the famous relation (V W )′ = W ′V ′, it is easily verified that H is a symmetric matrix. Also, some easy algebra shows that H is idempotent, i.e.
H2 = H (6.13) (The idempotencny also follows from the fact that H is a projection oper-
ator; once one projects, projecting the result won’t change it.) This leads us directly to the residuals:
L = D − Aβ􏰭 = D − HD = (I − H)D (6.14) The diagonal elements
hii = Hii (6.15) are known as the leverage values, another measure of influence like those in
Section 6.8.1, for the following reason. Looking at (6.12), we see that
D􏰭i = hiiDi (6.16)
This shows us the effect of true value Di on the fitted value D􏰭i:
hii = ∂D􏰭i (6.17)
∂Di
So, hii can be viewed as a measure of how much influence observation i has on its fitted value. A large value might thus raise concern — but how large is “large”?
Di = μ􏰭(Xi ) β
′
(6.11)
(6.12)

Let wi denote row i of H, which is also column i since H is symmetric. Then
n
hii = Hii = (H2)ii = wi′wi = h2ii + 􏰓 h2ij (6.18)
j =1,j ̸=i
This directly tells us that hii ≥ 0. But it also tells us that hii ≥ h2ii, which forces hii ≤ 1.
In other words,
0≤hii ≤1 (6.19) This will help assess whether a particular hii value is “large.”
As mentioned, there is a very large literature of this kind of analysis. The reader is referred to the many books on this topic, such as Atkinson and Riani (2000).
6.14.2 Martrix Inverse Update
The famous Sherman-Morrison-Woodbury formula says that for an invert- ible matrix B and vectors u and v
(B + uv′)−1 = B−1 − 1 B−1uv′B−1 (6.20) 1 + v′B−1u
In other words, if we have already gone to the trouble of computing a matrix inverse, and the matrix is then updated as above by adding uv′, then we do not have to compute the new inverse from scratch; we need only modify the old inverse, as specified above.
Let’s apply that to Section 6.8.1, where we discussed the effect of deleting an observation from our data set. Setting B = A′A, u = X􏰯i and v = −X􏰯i, then the new version of A′A after deleting observation i is
(A′A)(−i) = A′A + uv′ (6.21) That will be our value for the left-hand side of (6.20). Look what happens

to the right-hand side:
1 + v′B−1u = 1 − X􏰯i′(A′A)−1X􏰯i
But that subtracted term is just hii! Now that’s convenient. Now, again in (6.20), note that
(6.22)
(6.23)
(6.24)
and
B−1u = (A′A)−1X􏰯i
v′B−1 = −X􏰯i′(A′A)−1
After substuting all this in (6.20) to computer the new (A′A)−1, we can find our new β􏰭 by post-multiplying by A′(−i)D−i, where the latter factors are the new versions of A′ and D.
It should be noted, however, that this approach has poor roundoff error properties if A′A is ill-conditioned, meaning that it is nearly singular. This in turn can arise if some of the predictor variables are highly correlated with each other, as we will see in Chapter 8.
6.14.3 The Median Minimizes Mean Absolute Devia- tion
Let’s derive (6.8).
First, suppose a random variable W has a density fW . What value m
minimizes E[(W − m)]? 􏰞∞
E[(W −m)]
= |t−m| fV (t)dt −∞
􏰞m 􏰞∞
= (m−t) fV (t)dt+ −∞ m
(6.25) (t−m) fV (t)dt (6.26)
􏰞m 􏰞∞
= mP(W <m)− tfV(t)dt+ tfV(t)dt−mP(W >m) −∞ m

Differentiating with respect to m, we have
0 = P (W < m)+mfV (m)−mfV (m)−mfV (m)−[P (W > m)+m(−fV (m))] (6.27)
In other words,
P(W <m)=P(W >m) (6.28)
i.e. m = median(W ).
The extension to conditional median then follows the same argument as in
Section 1.13.1.
6.15 Further Exploration: Data, Code and Math Problems

Exercises

1. The contributors of the Forest Fire data set to the UCI Machine Learning Repository, https://archive.ics.uci.edu/ml/datasets/Forest+Fires, describe it as “a difficult regression problem.” Apply the methods of this chapter to attempt to tame this data set.

2. Using the methods of this chapter, re-evaluate the two competing Poisson-based analyses in Section 4.5.

3. Suppose the predictor vector X in a two-class classification problem has a joint density fX . Define the “population” residual r = Y − g(X), and its cumulative distribution function Fr.

(a) Express Fr as an integral involving fX and the regression function μ(t).

(b) Define a sample analog of (a), and use it to develop a graphical fit assessment tool for parametric models such as the logistic, using R’s empirical distribution function ecdf() and kNN.

4. Write an R function analogous to influence() for quantreg objects.

[1] The assumption of statistical independence was not covered there, and indeed will not be covered elsewhere in the book, in the sense of methods for assessing independence. The issue will of course be discussed in individual examples.

[2] “Rely on” here means that the method is not robust to the normality assumption.

[3] Or, at least, seem to have the strongest relation with the Yen. We cannot claim causation.

[4] We could of course simply use kNN in the first place. But this would not give us the Description usefulness of the parametric model, and also would give us a higher estimation variance.
