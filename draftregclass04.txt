Chapter 4
Nonlinear Models
Consider our bike-sharing data (e.g./ Section 1.12.2). Suppose we have several years of data. On the assumption that ridership trends are seasonal, and that there is no other time trend (e.g. no long-term growth in the program), then there would be a periodic relation between ridership R and G, the day in our data; here G would take the values 1, 2, 3, ..., with the top value being, say, 3 × 365 = 1095 for three consecutive years of data.1 Assuming that we have no other predictors, we might try fitting the model with a sine term:
mean R = β0 + β1 sin(2π · G/365) (4.1)
Just as adding a quadratic term didn’t change the linearity of our model in Section 1.11.1, the model (4.1) is linear too. In the notation of Section 2.3.2), as long as we can write our model as
mean D = A β (4.2) then by definition the model is linear. In the bike data model above, A
would be
 1 A= 1
sin(2π · 1/365)  sin(2π·2/365) 

(4.3)
 ... 1
sin(2π · 1095/365)
1We’ll ignore the issue of leap years here, to keep things simple.
97
￼
￼98 CHAPTER 4. NONLINEAR MODELS
But in this example, we have a known period, 365. In some other periodic setting, the period might be unknown, and would need to be estimated from our data. Our model might be, say,
mean Y = β0 + β1 sin(2π · X/β2) (4.4) where β2 is the unknown period. This does not correspond to (4.2). The
model is still parametric, but is nonlinear.
Nonlinear parametric modeling, then, is the topic of this chapter. We’ll de- velop procedures for computing least squares estimates, and forming con- fidence intervals and p-values, again without assuming homoscedasticity. The bulk of the chapter will be devoted to the Generalized Linear Model (GLM), which is a widely-used broad class of nonlinear regression mod- els. Two important special cases of the GLM will be the logistic model introduced briefly in Section 1.12.2, and Poisson regression.
4.1 Example: Enzyme Kinetics Model
Data for the famous Michaelis-Menten enzyme kinetics model is available in the nlstools package on CRAN. For the data set vmkm, we predict the reaction rate V from substrate concentration S. The model used was suggested by theoretical considerations to be
E(V |S=t)= β1t (4.5) β2 + t
In the second data set, vmkmki,2 an addiitonal predictor I, inhibitor concentration, was added, with the model being
E(V |S=t,I=u)= β1t (4.6) t+β2 (1+u/β3)
We’ll fit the model using R’s nls() function:
> library( nlstools )
> data(vmkmki)
> regftn <− function(t,u,b1,b2,b3)
b1 ∗ t / (t + b2 ∗ (1 + u/b3))
2There were 72 observation in this data, but the last 12 appear to be anomalous (gradient 0 in all elements), and thus were excluded.
￼￼￼
￼4.1. EXAMPLE: ENZYME KINETICS MODEL 99
All nonlinear least-squares algorithms are iterative: We make an initial guess at the least-squares estimate, and from that, use the data to update the guess. Then we update the update, and so on, iterating until the guesses converge. In nls(), we specify the initial guess for the parameters, using the start argument, an R list.3 Let’s set that up, and then run the analysis:
> bstart <− list (b1=1,b2=1, b3=1)
The values 1 here were arbitrary, not informed guesses at all. Domain
expertise can be helpful.
> z <− nls(v  ̃ regftn(S,I,b1,b2,b3),data=vmkmki, start=list (b1=1,b2=1, b3=1))
>z
Nonlinear regression model
model: v  ̃ regftn(S, I, b1, b2, b3) data : vmkmki
b1 b2 b3
18.06 15.21 22.28
sum−of−squares : 177.3 iterations to convergence : 11
convergence tolerance : 4.951e−06 So, β1 = 18.06 etc.
We can apply summary(), coef() and vcov() to the output of nls(), just as we did earlier with lm(). For example, here is the approximate covariance matrix of the coefficient vector:
> vcov(z)
b1 b2 b3 b1 0.4786776 1.374961 0.8930431 b2 1.3749612 7.568837 11.1332821 b3 0.8930431 11.133282 29.1363366
This assumes homoscedasticity. Under that assumption, an approximate 95% confidence interval for β1 would be
√
residual
􏰮
Number of Achieved
￼18.06 ± 1.96
0.4786776 (4.7)
￼3This also gives the code a chance to learn the names of the parameters, needed for computation of derivatives.
￼100 CHAPTER 4. NONLINEAR MODELS
One can use the approach in Section 3.3.3 to adapt nls() to the het- eroscedastic case, and we will do so in Section 4.2.2.
4.2 Least-Squares Computation
A point made in Section 1.3 was that the regression function, i.e. the con- ditional mean, is the optimal predictor function, minimizing mean squared prediction error. This still holds in the nonlinear (and even nonparametric) case. The problem is that in the nonlinear setting, the least-squares estima- tor does not have a nice, closed-form solution like (2.23) for the linear case. Let’s see how we can compute the solution through iterative approximation.
4.2.1 The Gauss-Newton Method
Denote the nonlinear model by
E(Y | X = t) = g(t,β) (4.8)
where both t and β are possibly vector-valued. In (4.5), for instance, t is a scalar but β is a vector. The least-squares estimate β􏰭 is the value of b that minimizes
n
􏰓[Yi − g(Xi, b)]2 (4.9) i=1
Many methods exist to minimize (4.9), most of which involve derivatives with respect to b. (The reason for the plural derivatives is that there is a partial derivative for each of the elements of b.)
The best intuitive explanation of derivative-based methods, which will also prove useful in a somewhat different context later in this chapter, is to set up a Taylor series approximation for g(Xi,b) (Appendix ??):
i i􏰭􏰭i􏰭􏰭
g(X , b) ≈ g(X , β) + h(X , β)′(b − β) (4.10)
where h(Xi, b) is the derivative vector of g(Xi, b) with respect to b, and the prime symbol, as usual, means matrix transpose (not a derivative). The
￼4.2. LEAST-SQUARES COMPUTATION 101
Table 4.1:
￼￼(4.13)
￼￼(2.12)
Yi − g(Xi, bk−1) + h(Xi, bk−1)′bk−1
￼￼￼Yi
￼h(Xi, bk−1)
X ̃i
￼￼￼￼￼value of β, is of course yet unknown, but let’s put that matter aside for now. Then (4.9) is approximately
􏰭
n
􏰓[Yi − g(Xi, β) + h(Xi, β)′β − h(Xi, β)′ b]2 i=1
(4.11)
to β, and make that substitution in (4.11), yielding
􏰭􏰭􏰭􏰭
At iteration k we take our previous iteration bk−1 to be an approximation 􏰭
n
􏰓[Yi − g(Xi, bk−1) + h(Xi, bk−1)′bk−1 − h(Xi, bk−1)′ b]2 (4.12) i=1
Our bk is then the value that minimizes (4.12) over all possible values of b. But why is that minimization any easier than minimizing (4.9)? To see why, write (4.12) as
n
􏰓[Yi − g(Xi, bk−1) + h(Xi, bk−1)′bk−1 −h(Xi, bk−1)′ b]2 (4.13)
i=1 􏰎 ￼ 􏰏􏰐 ￼ 􏰑
This should look familiar. It has exactly the same form as (2.12), with the correspondences shown in Table 4.1. In other words, what we have in (4.13) is a linear regression problem!
In other words, we can find the minimizing b in (4.13) using lm(). There is one small adjustment to be made, though. Recall that in (2.12), the quantity X􏰯i includes a 1 term (Section 2.1), i.e. the first column of A in (2.13) consists of all 1s. That is not the case in Table 4.1 (second row, first column), which we need to indicate in our lm() call. We can do this via specifying “-1” in the formula part of the call.
￼1
2
3
4
5
6
7
8 9#
# 10 #
arguments :
nlslmout : return value from
nlsLM()
11 #
12 #
13 #
14
value: approximate covariance matrix for the estimated parameter vector
102 CHAPTER 4. NONLINEAR MODELS
Another issue is the computation of h(). Instead of burdening the user with with this, it is typical to compute h() using numerical approximation, e.g. using R’s numericDeriv() function or the numDeriv package.
4.2.2 Eickert-White Asymptotic Standard Errors
As noted, nls() assumes homoscedasticity, which generally is a poor as- sumption (Section 2.4.2). It would be nice, then, to apply the Eickert- White method (Section 3.3.3). Actually, it is remarkably easy to adapt that method to the nonlinear computation above.
The key is to note the linear approximation (4.2.1). One way to look at this is that it has already set things up for us to use the delta method, which uses a linear approximation. Thus we can apply Eickert-White to the lm() output, say using vcovHC(), as in Section 3.3.4.
Below is code along these lines. It requires the user to run nlsLM(), an alternate version of nls() in the CRAN package minpack.lm.4
library(minpack.lm) library ( sandwich )
# # #
uses output of nlsLM() of the minpack.lm package to get an asymptotic covariance matrix without assuming homoscedasticity
15 nlsvcovhc <− function ( nlslmout ) {
16 # notation: g(t,b) is the regression model,
17 # where x is the vector of variables for a
18 # given observation ; b is the estimated parameter
19 # vector; x is the matrix of predictor values
20 b <− coef ( nlslmout )
21 m <− nlslmout$m
4This version is needed here because it provides the intermediate quantities we need from the computation. However, we will see in Section 4.2.4 that this version has other important advantages as well.
￼
￼22
23
24
25
26
27
28
29
30
31
32
33
34
}
4.2.
LEAST-SQUARES COMPUTATION 103
#y−g:
r e s i d <− m$ r e s i d ( )
# row i of hmat will be deriv of g(x[i ,] ,b) # with respect to b
hmat <− m$gradient ()
# calculate the artificial ”x” and ”y” of # the algorithm
fakex <− hmat
fakey <− resid + hmat %∗% b
# −1 means no constant term in the model
lmout <− lm(fakey  ̃ fakex − 1)
vcovHC(lmout)
In addition to nice convergence behavior, the advantage for us here of nl- sLM() over nls() is that the former gives us access to the quantities we need in (4.13), especially the matrix of h() values. We then apply lm() one more time, to get an object of type ”lm”, needed by vcovHC().
Applying this to the enzyme data, we have
> nlsvcovhc(z)
fakex1 fakex2
fakex1 0.4708209 1.706591 fakex2 1.7065910 10.394496 fakex3 2.4107117 20.314688
This is rather startling. Except for
mated variances and covariances from Eickert-White are much larger than what nls() found under the assumption of homoscedasticity.
Of course, with only 60 observations, both of the estimated covariance matrices must be “taken with a grain of salt.” So, let’s compare the two approaches by performing a simulation. Here
E(Y |X=t)= 1 (4.14) t′β
where t = (t1,t2)′ and β = (β1,β2)′. We’ll take the components of X to be independent and exponentially distributed with mean 1.0, with the heteroscedasticity modeled as being such that the standard deviation of Y given X is proportional to the regression function value at X. We’ll use as a check the fact that a N(0,1) variable is less than 1.28 90% of the time, Here is the simulation code:
fakex3 2.410712 20.314688 53.086958
the estimated variance of β1, the esti-
􏰭
￼
￼104 CHAPTER 4.
sim <− function(n,nreps) { b <− 1 : 2
NONLINEAR MODELS
res <− replicate(nreps,{
x <− matrix(rexp(2∗n),ncol=2)
meany <− 1 / (x %∗% b)
y <− meany + (runif(n) − 0.5) ∗ meany xy <− cbind(x,y)
xy <− data.frame(xy)
nlout <− nls(X3  ̃ 1 / (b1∗X1+b2∗X2),
data=xy,start=list(b1 = 1,b2=1)) b <− coef(nlout)
vc <− vcov(nlout)
vchc <− nlsvcovhc(nlout)
z 1 <− ( b [ 1 ] − 1 ) / s q r t ( v c [ 1 , 1 ] )
z 2 <− ( b [ 1 ] − 1 ) / s q r t ( v c h c [ 1 , 1 ] ) c ( z1 , z2 )
})
print(mean(res[1,] < 1.28)) print(mean(res[2,] < 1.28))
}
And here is a run of the code:
> sim(250,2500) [1] 0.6188
[1] 0.9096
That’s quite a difference! Eickert-White worked well, whereas assuming homoscedasticity fared quite poorly. (Similar results were obtained even for n = 100.)
4.2.3 Example: Bike Sharing Data
In our bike-sharing data (Section 1.12.2), there are two kinds of riders, registered and casual. We may be interested in factors determining the mix, i.e.
registered registered + casual
(4.15)
￼Since the mix proportion is between 0 and 1, we might try the logistic model, introduced in (4.18), in the context of classification, though the
￼4.2. LEAST-SQUARES COMPUTATION 105
example here does not involve a classification problem, and use of glm() would be inappropriate. Here are the results:
> > > > > > >
>
shar <− read.csv(”day.csv”,header=T) shar$temp2 <− shar$tempˆ2
shar$summer <− as.integer(shar$season == 3) shar$propreg <− shar$reg / ( shar$reg+shar$cnt )
names(shar)[15] <− ”reg”
library(minpack.lm)
logit <− function(t1 ,t2 ,t3 ,t4 ,b0,b1,b2,b3,b4)
1 / (1 + exp(−b0 − b1∗t1 −b2∗t2 −b3∗t3 −b4∗t4)) z <− nlsLM(propreg  ̃
l o g i t ( temp , temp2 , workingday , summer , b0 , b1 , b2 , b3 , b4 ) ,
data=shar > summary(z)
... Parameters :
, start=list (b0=1,b1=1,b2=1,b3=1,b4=1))
b0 b1 b2 b3 b4
...
Estimate −0.083417 −0.876605
0.563759 0.227011 0.012641
Std . Error 0.020814 0.093773 0.100890 0.006106 0.009892
t value −4.008 −9.348
5.588 37.180 1.278
Pr(>|t |) 6.76e−05 ∗∗∗
< 2e−16 ∗∗∗ 3.25e−08 ∗∗∗ < 2e−16 ∗∗∗
0.202
As expected, on working days, the proportion of registered riders is higher, as we are dealing with the commute crowd on those days. On the other hand, the proportion doesn’t seem to be much different during the summer, even though the vacationers would presumably add to the casual-rider ount.
But are those standard errors trustworthy? Let’s look at the Eickert-White versions:
> sqrt(diag(nlsvcovhc(z)))
fakex1 fakex2 fakex3 fakex4
0.021936045 0.090544374 0.092647403 0.007766202 fakex5
0.007798938
Again, we see some substantial differences.
￼106 CHAPTER 4. NONLINEAR MODELS
4.2.4 The “Elephant in the Room”: Convergence Is- sues
So far we have sidestepped the fact that any iterative method runs the risk of nonconvergence. Or it might converge to some point at which there is only a local minimum, not the global one — worse than nonconvergence, in the sense that the user might be unaware of the situation.
For this reason, it is best to try multiple, diverse sets of starting values. In addition, there are refinements of the Gauss-Newton method that have better convergence behavior, such as the Levenberg-Marquardt method.
Gauss-Newton sometimes has a tendency to “overshoot,” producing too large an increment in b from one iteration to the next. Levenberg-Marquardt generates smaller increments. Interestingly it is a forerunner of ridge re- gression that we’ll discuss in Chapter 8. It is implemented in the CRAN package minpack.lm, which we used earlier in this chapter.
4.2.5 Example: Eckerle4 NIST Data
The U.S. National Institute of Standards and Technology has available sev- eral data sets that serve as test beds for nonlinear regression modeling. The one we’ll use here is called Eckerle4.5 The data are from a NIST study in- volving circular interference transmittance. Here we predict transmittance from wavelength, with a model like a normal density:
meantransmittance=β1 exp[−0.5(wavelength−β3)2] (4.16) β2 β2
NIST also provides sets of starting values. For this data set, the two sug-
gested starting vectors are (1,10,500) and (1.5,5,450), values that appar-
ently came from informal inspection of a plot of the data, seen in Figure
4.1. It is clear, for instance, that β3 is around 450. The standard deviation
of a normal curve is the distance from the center of the curve to either
inflection point, say about 10 here. Since since the maximum value of the
curve is about 0.4, we can then solve for an initial guess for β1.
It turns out that ordinary nls() works for the second set but not the first:
> eck <− read.table(”Eckerle4.dat”,header=T)
> frm <− y  ̃ (b1/b2) ∗ exp(−0.5∗((x−b3)/b2)ˆ2)
5See http://www.itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Eckerle4.dat.
￼￼􏰭
􏰭
￼
￼4.2. LEAST-SQUARES COMPUTATION 107
￼...
> bstart
$b1 [1] 1
$b2
[1] 10
$b3
[1] 500
Figure 4.1: Eckerle4 Data
> nls(frm,data=eck,start=bstart)
Error in nls (frm , data = eck , start = bstart ) : singular gradient
> bstart <− list (b1=1.5, b2=5, b3=450) > nls(frm,data=eck,start=bstart)
Nonlinear regression model
model: y  ̃ (b1/b2) ∗ exp(−0.5 ∗ ((x − b3)/b2)ˆ2)
data: eck
b1 b2 b3
1.554 4.089 451.541
residual sum−of−squares : 0.001464
￼108
Number of Achieved
iterations convergence
CHAPTER 4. NONLINEAR MODELS
to convergence : 6 tolerance : 1.395e−06
But nlsLM() worked with both sets
4.2.6 The Verdict
Nonlinear regression models can be powerful, but may be tricky to get to converge properly. Moreover, convergence might be achieved at a local, rather than global minimum, in which case the statistical outcome may be problematic. A thorough investigation of convergence (and fit) issues in any application is a must.
Fortunately, for the special case of Generalized Linear Models, the main focus of this chapter, convergence is rarely a problem. So, let’s start dis- cussing GLM.
4.3 The Generalized Linear Model
Recall once again the logistic model, introduced in (4.18). We are dealing with a classification problem, so the Y takes on the values 0 and 1. Let X = (X(1), X(2), ..., X(p))′ denote the vector of our predictor variables.
4.3.1 Definition
Our model is
P(Y =1|X(1) =t1,...,X(p)) =tp)=l(β0+β1t1+...+βptp) where
l(s) = 1 1+e−s
(4.17)
(4.18)
￼and t = (t1, ..., tp)′.6
6Recall from Section 1.12.1 that te classification problem is a special case of regression.
￼
￼4.3. THE GENERALIZED LINEAR MODEL 109
The key point is that even though the right-hand side of (4.17) is not linear in t, it is a function of a linear expression in t, hence the term generalized linear model (GLM).
So, GLM is actually a broad class of models. We can use many different functions q() in place of l() in (4.18); for each such function, we have a different GLM.
4.3.2 Example: Poisson Regression
For example, setting q() = exp() gives us a model known as Poisson regres- sion, which assumes
E(Y = 1 | X(1) = t1,...,X(p)) = tp) = eβ0+β1t1+...+βptp (4.19)
In addition, GLM assumes a specified parametric class for the conditional distribution of Y given X , which we will denote FY |X . In Poisson regression, this assumption is, not surprisingly, that the conditional distribution of Y given X is Poisson. In the logistic case, the assumption is trivially that the distribution is Bernoulli, i.e. binomial with number of trials equal to 1. Having such assumptions enables maximum likelihood estimation.
In particular, the core of GLM assumes that FY |X belongs to an exponential family. This is formally defined as a parametric family whose probability density/mass function has the form
exp[η(θ)T (x) − A(θ) + B(x)] (4.20)
where θ is the parameter vector and x is a value taken on by the random variable. Though this may seem imposing, it sufficies to say that the above formulation includes many familiar distribution families such as Bernoulli, binomial, Poisson, exponential and normal. In the Poisson case, for in- stance, setting η(θ) = logλ, T(x) = k, A(θ) = −λ and B(x) = −log(k!) yields the expression
e−λ λk k!
the famous form of the Poisson probability mass function.
(4.21)
￼GLM terminology centers around the link function, which is the functional inverse of our function q() agove. For Poisson regression, the link function
￼110 CHAPTER 4. NONLINEAR MODELS
is the inverse of exp() i.e. log(). For logit, set u = l(s) = (1 + exp(−s))−1, and solve for s, giving us the link function,
link(u) = u (4.22) 1−u
4.3.3 GLM Computation
Though estimation in GLM uses maximum likelihood, it can be shown that the actual computation can be done extending the ideas in Section 4.2, i.e. via least-squares models. The only new aspect is the addition of a weight function, which works as follows.
Let’s review Section 3.11, which discussed weighted least squares in the case of a linear model. Using our usual notation μ(t) = E(Y | X = t) and σ2(t) = V ar(Y | X = t), the optimal estatimor of β is the value of b that minimizes
￼n
􏰓
1 (Y−X′b)2
(4.23)
￼σ 2 ( X􏰰 ) i=1 i
􏰰
Now consider the case of Poisson regression. One of the famous properties of the Poisson distribution family is that the variance equals the mean. Thus (4.9) becomes
􏰓n 1 i=1 g(Xi,b)
[Yi −g(Xi,b)]2
(4.24)
ii
￼Then (4.13) bceoms
n
1
􏰓
i=1 g(Xi,bk−1) 􏰎 ￼ 􏰏􏰐 ￼ 􏰑
[Yi − g(Xi, bk−1) + h(Xi, bk−1)′bk−1 −h(Xi, bk−1)′ b]2 (4.25)
￼and we again solve for the new iterate bk by calling lm(), this time making use of the latter’s weights argument.
We iterate as before, but now the weights are updated at each iteration too. For that reason, the process is known as iteratively reweighted least squares.
￼4.3. THE GENERALIZED LINEAR MODEL 111
4.3.4 R’s glm() Function
Of course, the glm() function does all this for us. For ordinary usage, the call is the same as for lm(), except for one extra argument, family. In the Poisson regression case, for example, the call looks like
glm(y  ̃ x, family = poisson)
The family argument actually has its own online help entry:
> ?family
family package: stats
R Documentation
Family Objects for Models
Description : ...
Usage :
family(object , ...)
binomial(link = ”logit”)
gaussian(link = ”identity”) Gamma(link = ”inverse”)
inverse.gaussian(link = ”1/muˆ2”)
poisson(link = ”log”)
quasi(link = ”identity”, variance = ”constant”) quasibinomial(link = ”logit”)
quasipoisson(link = ”log”)
...
Ah, so the family argument is a function! There are built-in ones we can use, such as the poisson one we used above, or a user could define her own custom function.
Well, then, what are the arguments to such a function? A key argument is link, which is obviously the link function q() discussed above, which we found to be log() in the Poisson case.
For a logistic model, as noted earlier, FY |X is binomial with number of trials m equal to 1. Recall that the variance of a binomial random variable with m trials is mr(1−r), where r is the “success” probability on each trial,
￼112 CHAPTER 4. NONLINEAR MODELS
Recall too that the mean of a 0-1-valued random variable is the probability of a 1. Putting all this together, we have
σ2(t) = μ(t)[1 − μ(t)] (4.26) Sure enough, this appears in the code of the built-in function binomial():
> binomial
function (link = ”logit”) {
...
variance <− function(mu) mu ∗ (1 − mu)
Let’s now turn to details of two of the most widely-used models, the logistic and Poisson.
4.4 GLM: the Logistic Model
The logistic regression model, introduced in Section 1.12.2, is by far the most popular nonlinear regression method. Here we are predicting a re- sponse variable Y that takes on the values 1 and 0, indicating which of two classes our unit belongs to. As we saw in Section 1.12.1, this indeed is a regression situation, as E(Y | X = t) reduces to P(Y = 1 | X = t).
The model, again, is
P(Y = 1 | X = (t1,...,tp)) = 1 (4.27)
4.4.1 Motivation
We noted in Section 1.12.2 that the logistic model is appealing for two reasons: (a) It takes values in [0,1], as a model for probabilities should, and (b) it is monotone in the predictor variables, as in the case of a linear model.
But there’s even more. It turns out that the logistic model is related to many familiar distribution families. We’ll show that in this section. In ad- dition, the derivations here will deepen the reader’s insight into the various conditional probabilities involved in the overall classification problem.
￼1 + e−(β0+β1t1+....+βptp)
￼4.4. GLM: THE LOGISTIC MODEL 113
To illustrate that, consider a very simple example of text classification, involving Twitter tweets. Suppose we wish to automatically classify tweets into those involving financial issues and all others. We’ll do that by having our code check whether a tweet contains words from a list of financial terms we’ve set up for this purpose, say bank, rate and so on.
Here Y is 1 or 0, for the financial and nonfinancial classes, and X is the number of occurrences of terms from the list. Suppose that from past data we know that among financial tweets, the number of occurrences of words from this list has a Poisson distribution with mean 1.8, while for nonfinancial tweets the mean is 0.2. Mathematically, that says that FX|Y =1 is Poisson with mean 1.8, and FX|Y =0 is Poisson with mean 0.2. (Be sure to distinguish the situation here, in which FX|Y is a Poisson distribution, from Poisson regression (Section 4.3.2), in which it is assumed that FY |X is Poisson.) Finally, suppose 5% of all tweets are financial.
Recall once again (Section 1.12.1) that in the classification case, our regres- sion function takes the form
μ(t)=P(Y =1|X=t) Let’s calculate this function:
P(Y =1|X=t) = P(Y =1andX=t) P X = t)
P(Y =1andX=t)
P(Y =1andX=torY =1andX=t)
(4.28)
(4.29)
￼= =
where π = P (Y = 1) is the population proportion of individuals in class 1. The numerator in (4.29) is
￼π P (X = t | Y = 1) πP(X=t|Y =1)+(1−π)P(X=t|Y =0)
￼e−1.8 1.8t 0.05 · t!
and similarly the denominator is
e−1.8 1.8t e−0.2 0.2t 0.05 · t! + 0.95 · t!
(4.30)
(4.31)
￼￼￼
￼114 CHAPTER 4. NONLINEAR MODELS
Putting this into (4.29) and simplifying, we get
P(Y=1|X=t) = 1 1+19e 9
(4.32) (4.33)
(4.34)
(4.35)
￼1.6( 1 )t =1
￼￼1+exp(log19−0.2−tlog9)
That last expression is of the form
1
￼with
and
1 + exp[−(β0 + β1t)]
β0 = −log19+0.2
β1 =log9
In other words the setting in which FX|Y is Poisson implies the logistic
model!
This is true too if FX|Y is an exponential distribution. Since this is a continuous distribution family rather than a discrete one, the quantities P (X = t|Y = i) in (4.32) must be replaced by density values:
P(Y =1|X=t)= π f1(X = t | Y = 1)
πf1(X=t|Y =1)+(1−π)f0(X=t|Y =0) where the within-class densities of X are
(4.37)
(4.36)
￼fi(w) = λie−λiw, i = 0, 1 (4.38) After simplifying, we again obtain a logistic form.
￼4.4. GLM: THE LOGISTIC MODEL 115
Most important, consider the multivariate normal case: Say for groups i = 0, 1, FX |Y =i is a multivariate normal distribution with mean vector μi and covariance matrix Σ, where the latter does not have a subscript i. This is a generalization of the classical two-sample t-test setting, in which two (scalar) populations are assumed to have possibly different means but the same variance.7 Again using (4.37), and going through a lot of algebra, we find that again P(Y = 1 | X = t) turns out to have a logistic form,
￼￼with
and
P(Y =1|X=t)= 1
1 + e−(β0+β′t)
β0 =log(1−π)−logπ+1(μ′1μ1−μ′0μ0) 2
β = (μ0 − μ1)′Σ−1
(4.39)
(4.40)
(4.41)
￼￼where t is the vector of predictor variables, the β vector is broken down into (β0,β), and π is P(Y = 1). The messy form of the coefficients here is not important; instead, the point is that we find that the multivariate normal model implies the logistic model, giving the latter even more justification.
In summary:
Not only is the logistic model intuitively appealing because it is a monotonic function with values in (0,1), but also because it is implied by various familiar parametric models for the within- class distribution of X.
No wonder the logit model is so popular!
4.4.2 Example: Pima Diabetes Data
Another famous UCI data set is from a study of the Pima tribe of Native Americans, involving factors associated with diabetes. There is data on 768
7It is also the setting for Fisher’s Linear Discriminant Analysis, to be discussed in Section ??.
￼￼
￼116 CHAPTER 4. NONLINEAR MODELS
women.8 Let’s preduct diabetes from the other variables:
> logitout <− glm(Diab > summary( logitout )
... Coefficients :
Estimate ( Intercept ) −8.4046964 NPreg 0.1231823 Gluc 0.0351637 BP −0.0132955 Thick 0.0006190 Insul −0.0011917 BMI 0.0897010 Genet 0.9451797 Age 0.0148690
 ̃ . ,data=pima,family=binomial)
( Intercept ) NPreg
Gluc
BP Thick Insul
BMI Genet Age
...
Pr(>|z |)
< 2e−16 ∗∗∗
0.000123 ∗∗∗ < 2e−16 ∗∗∗
0.011072 ∗ 0.928515 0.186065 2.76e−09 ∗∗∗ 0.001580 ∗∗ 0.111192
Std . Error 0.7166359 0.0320776 0.0037087 0.0052336 0.0068994 0.0009012 0.0150876 0.2991475 0.0093348
z value −11.728 3.840 9.481 −2.540 0.090 −1.322 5.945 3.160 1.593
4.4.3 Interpretation of Coefficients
In nonlinear regression models, the parameters βi do not have the simple marginal interpretation they enjoy in the linear case. Statements like we made in Section 1.7.1.2, “We estimate that, on average, each extra year of age corresponds to almost a pound in extra weight,” are not possible here.
However, in the nonlinear case, the regression function is still defined as the conditional mean, or in the logit case, the conditional probability of a 1. Practical interpretation is definitely still possible, if slightly less convenient.
Consider for example the estimated Glucose coefficient in our diabetes data
8 The data set is at https://archive.ics.uci.edu/ml/datasets/Pima+Indians+ Diabetes. I have added a header record to the file.
￼
￼4.4. GLM: THE LOGISTIC MODEL 117
above, 0.035. Let’s apply that to the people similar to the first person in the data set:
> pima[1,]
NPreg Gluc BP Thick I n s u l BMI Genet Age Diab
1 6 148 72 35 0 33.6 0.627 50 1
Ignore the fact that this woman has diabetes. Let’s consider the population group of all women with the same characteristics as this one, i.e. all who have had 6 pregnancies, a glucose level of 148 and so on, through an age of 50. The estimated proportion of women in this population group is
1
1 + e−(8.4047+0.1232·6+...+0.0149·50)
We don’t have to plug these numbers in by hand, of course:
(4.42)
￼> l <− function(t) 1/(1+exp(−t))
> pima1 <− pima[1,−9] # exclude diab. var.
> pima1 <− unlist(pima[1,−9]) # had been a data frame > l (coef( logitout ) %∗% c(1 ,pima1))
[ ,1] [1 ,] 0.7217266
So, about 72% of women in this population group have diabetes. But what about the population group of the same characteristics, but of age 40 instead of 50?
> w <− pima1
> w [ ’ A g e ’ ] <− 4 0
> l (coef( logitout ) %∗% c(1 ,w))
[ ,1] [1 ,] 0.6909047
Only about 69% of the younger women have diabetes.
So, there is an effect of age on developing diabetes, but only a mild one; a 10-year increase in age only increased the chance of diabetes by about 3.1%. However, note carefully that this was for women having a given set of the other factors, e.g. 6 pregnancies. Let’s look at a different population group, those with 2 pregnancies and a glucose level of 120, comparing 40- and 50-year-olds:
> u <− pima1 > u [ 1 ] <− 2
￼118 CHAPTER 4.
> u[2] <− 100
> v <− u
> v[8] <− 40
> l (coef( logitout ) %∗% c(1 ,u))
[ ,1] [1 ,] 0.2266113
> l (coef( logitout ) %∗% c(1 ,v)) [ ,1]
[1 ,] 0.2016143
NONLINEAR MODELS
So here, the 10-year age effect was somewhat less, about 2.5%. A more careful analysis would involve calculating standard errors for these numbers, but the chief point here is that the effect of a factor in nonlinear situations depends on the values of the other factors.
P(Y =0|X(1) =t1,...,X(p)) =tp)=1−l(β0+β1t1+...+βptp) (4.43) Some analysts like to look at the log-odds ratio,
P(Y = 1 | X(1) = t1,...,X(p)) = tp) P(Y = 0 | X(1) = t1,...,X(p)) = tp)
(4.44)
￼in this case the ratio of the probability of having and not having the disease. By Equation (4.17), this simplifies to
β0 + β1t1 + ... + βptp (4.45)
— a linear function! Thus, in interpreting the coefficients output from a logisitic analysis, it is convenient to look at the log-odds ratio, as it gives us a single number for each factor. This may be sufficient for the application at hand, but a more thorough analysis should consider the effects of the factors on the probabilities themselves.
4.4.4 The predict() Function
IN the previous section, we evaluated the estimated regression function (and thus predicted values as well) the straightforward but messy way, e.g.
> l (coef( logitout ) %∗% c(1 ,v))
￼4.4. GLM: THE LOGISTIC MODEL 119
The easy way is to use R’s predict() function:
> predict(object=logitout ,newdata=pima[1,−9], type=’ response ’ )
1 0.7217266
Let’s see what just happened. First, predict() is what is called a generic function in R. What this means is that it is not a single function, but rather an umbrella leading into a broad family of functions, depending on the class of the object on which it is invoked.
In our case here, we invoked it on logitout. What is the class of that object?
> class(logitout) [ 1 ] ”glm” ”lm”
So, it is an object of class ”glm” (and, we see, the latter is a subclass of the class ”lm”).
In processing our call to predict(), the R interpreter found that our ob- ject had class ”glm”, and thus looked for a function predict.glm(), which the authors of glm() had written for us. So, in R terminology, the interpreter dispatched, i.e. relayed, our priedct()call to predict.glm(). What did the latter then do?
The argument newdata is a data frame consisting of what its name implies — new cases to predict. As you will recall, the way we predict a new case in regression analysis is to take as our prediction the value of the regression function for that casew. The net result is then that predict() is giving us the value of the regression function at our requested points, in this case for the population group of all women with the same traits as the first person in our data.
So, predict() while doesn’t give us any new capabilities, it certainly makes things more convenient. The reader should ponder, for instance, how to use this function to simplify the code in Section 1.9.4.1.
4.4.5 Linear Boundary
In (4.27), which values of t = (t1,...,tp)′ will cause us to gues Y = 1 and which will result in a guess of Y = 0? The boundary occurs when (4.27)
￼120 CHAPTER 4. NONLINEAR MODELS
has the value 0.5. In other words, the boundary consists of all t such that β0 +β1t1 +....+βptp =0 (4.46)
So, the boundary has linear form, a hyperplane in p-dimensional space. This may seem somewhat abstract now, but it will have value later on.
4.5 GLM: the Poisson Model
Since in the above data the number of pregnancies is a count, we might consider predicting it using Poission regression. (The reader may wonder why we may be interested in such a “reverse” prediction. It could occur, for instance, with multiple imputation methods to deal with missing data.) Here’s how we can do this with glm():
> poisout <− glm(NPreg > summary( poisout )
... Coefficients :
 ̃ . ,data=pima,family=poisson)
Estimate 0.2963661 −0.0015080 0.0011986 0.0000732 −0.0003745 −0.0002781 −0.1664164 Age 0.0319994 Diab 0.2931233
Pr(>|z |) (Intercept) 0.01408 ∗ Gluc 0.02450 ∗
BP 0.25419 Thick 0.95604 Insul 0.04801 ∗
Std . Error 0.1207149 0.0006704 0.0010512 0.0013281 0.0001894 0.0027335 0.0606364 0.0014650 0.0429765
z
value
2.455 −2.249 1.140 0.055 −1.977 −0.102 −2.744 21.843 6.821
( Intercept ) Gluc
BP Thick Insul
BMI Genet
BMI 0.91896 Genet 0.00606 ∗∗ Age < 2e−16 ∗∗∗ Diab 9.07e−12 ∗∗∗
...
￼4.5. GLM: THE POISSON MODEL 121
On the other hand, even if we believe that our count data follow a Poisson distribution, there is no law dictating that we use Poisson regression, i.e. the model (4.3.2). The main motivation for using exp() in that model is to ensure that our regression function is nonnegative, conforming to the non- negative nature of Poisson random variables. This is not unreasonable, but as noted in a somewhat different context in Section 3.3.7, transformations like this can produce distortions. Let’s try an alternative:
> quasiout <− glm(NPreg  ̃ .,data=pima, family=quasi( variance=”muˆ2”) , start=rep(1 ,9))
This “quasi” family is a catch-all option, specifying a linear model but here allowing us to specify a Poisson variance function.
Well, then, which model performed better? As a rough, quick look, ignoring issues of overfitting and the like, let’s consider R2. This quantity is not calculated by glm(), but recall from Section 2.7.2 that R2 is the squared correlation between the predicted and actual Y values. This quantity makes sense for any regression situation, so let’s calculate it here:
> cor( poisout$fitted . values , poisout$y)ˆ2 [1] 0.2314203
> cor( quasiout$fitted . values , quasiout$y)ˆ2 [1] 0.3008466
The “unorthodox” model performed better. We cannot generalize from this, but it does show again that one must use transformations carefully.
￼122 CHAPTER 4. NONLINEAR MODELS