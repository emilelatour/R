---
title: "draftregclass01"
author: "Robert A. Stevens"
date: "August 28, 2016"
output: html_document
---

*From Linear Models to Machine Learning: Regression and Classification, with R Examples*

by Norman Matloff

# Chapter 1: Setting the Stage

This chapter will set the stage, previewing many of the major concepts to be presented in later chapters. The material here will be referenced repeatedly throughout the book.

## 1.1 Example: Predicting Bike-Sharing Activity

Let’s start with a well-known dataset, Bike Sharing, from the Machine Learning Repository at the University of California, Irvine [1]. Here we have daily-hourly data on the number of riders, weather conditions, day-of-week, month and so on. Regression analysis may turn out to be useful to us in at least two ways:

- Prediction: The managers of the bike-sharing system may wish to predict ridership, say for the following question:

Tomorrow, Sunday, is expected to be sunny and cool, say 62 degrees Fahrenheit. We may wish to predict the number of riders, so that we can get some idea as to how many bikes will need repair. We may try to predict ridership, given the weather conditions, day of the week, time of year and so on.

- Description: We may be interested in determining what factors affect ridership. How much effect, for instance, does wind speed have in influencing whether people wish to borrow a bike?

These twin goals, Prediction and Description, will arise frequently in this book. Choice of methodology will often depend on the goal in the given application.

## 1.2 Example: Body fat Prediction

The great baseball player, Yogi Berra was often given to malapropisms, one of which was supposedly his comment, “Prediction is difficult, especially about the future.” But there is more than a grain of truth to this, because indeed we may wish to “predict” the present or even the past.

For example, consider the bodyfat data set, available in the R package, mfp. Body fat is expensive and unwieldy to measure directly, as it involves underwater weighing. Thus it would be highly desirable to “predict” that quantity from easily measurable variables such as height, age, weight, abdomen circumference and so on.

In scientific studies of ancient times, there may be similar situations in which we “predict” unknown quantities from known ones.

## 1.3 Optimal Prediction

Even without any knowledge of statistics, many people would find it reasonable to predict via subpopulation means. In the above bike-sharing example, say, this would work as follows.

Think of the “population” of all days, past, present and future, and their associated values of number of riders, weather variables and so on [2]. Our data set is considered a sample from this population. Now consider the subpopulation consisting of all days with the given conditions: Sundays, sunny skies and 62-degree-temperatures.

It is intuitive that:

A reasonable prediction for tomorrow’s ridership would be the mean ridership among all days in the subpopulation of Sundays with sunny skies and 62-degree-temperatures.

In fact, such a strategy is optimal, in the sense that it minimizes our expected squared prediction error. We will defer the proof to Section 1.13.1 in the Mathematical Complements section at the end of this chapter, but what is important for now is to note that in the above prediction rule, we are dealing with conditional means: This is mean ridership, given day of the week is Sunday, sky conditions are sunny, and temperature is 62.

## 1.4 A Note About E(), Samples and Populations

To make this more mathematically precise, keep in mind that in this book, as with many other books, the expected value functional E() refers to population mean. Say we are studying personal income, I, for some population, and we choose a person at random from that population. Then E(I) is not only the mean of that random variable, but much more importantly, it is the mean income of all people in that population.

Similarly, we can define condition means, i.e., means of subpopulations. Say G is gender. Then the conditional expected value, E(I | G = male) is the mean income of all men in the population.

To illustrate this in the bike-sharing context, let’s define some variables:

- R, the number of riders

- W, the day of the week

- S, the sky conditions, e.g. sunny 

- T, the temperature

We would like our prediction R to be [3] the conditional mean,

    R = E(R | W = Sunday, S = sunny, T = 62)  (1.1)

There is one major problem, though: We don’t know the value of the right-hand side of (1.1). All we know is what is in our sample data, whereas the right-side of (1.1) is a population value, and thus unknown.

The difference between sample and population is of course at the very core of statistics. In an election opinion survey, for instance, we wish to know p, the proportion of people in the population who plan to vote for Candidate Jones. But typically only 1200 people are sampled, and we calculate the proportion of Jones supporters among them, p, and then use that as our estimate of p.

Similarly, though we would like to know the value of E(R | W = Sunday, S = sunny, T = 62), it is an unknown population value, and thus must be estimated from our sample data, which we’ll do later in this chapter.

Readers will greatly profit from constantly keeping in mind this distinction between populations and samples.

Before going on, a bit of terminology: We will refer to the quantity to be predicted, e.g. R above, as the response variable, and the quantities used in prediction, e.g. W, S and T above, as the predictor variables. (By the way, the machine learning community uses the term features rather than predictors.)

## 1.5 Example: Do Baseball Players Gain Weight As They Age?

Though the bike-sharing data set is the main example in this chapter, it is rather sophisticated for introductory material. Thus we will set it aside temporarily, and bring in a simpler data set for now. We’ll return to the bike-sharing example in Section 1.10.

This new dataset involves 1015 major league baseball players, courtesy of the UCLA Statistics Department. You can obtain the data either from the UCLA Web page, or as the data set mlb in freqparcoord, a CRAN package authored by Yingkang Xie and myself. The variables of interest to us here are player weight W, height H and age A, especially the first two.

Here are the first few records:

```{r comment=NA}
library(freqparcoord) 
data(mlb)
head(mlb)
```

### 1.5.1 Prediction vs. Description

Catcher Catcher Catcher

Recall the Prediction and Description goals of regression analysis, discussed in Section 1.12.2. With the baseball player data, we may be more interested in the Description goal, such as:

Athletes strive to keep physically fit. Yet even they may gain weight over time, as do people in the general population. To what degree does this occur with the baseball players? This question can be answered by performing a regression analysis of weight against height and age, which we’ll do in Section 1.7.1.2.

On the other hand, there doesn’t seem to be much of a Prediction goal here. It is hard to imagine a need to predict a player’s weight. However, for the purposes of explaining the concepts, we will often phrase things in a Prediction context. This is somewhat artificial, but it will serve our purpose of introducing the basic concepts in the very familiar setting of human characteristics.

So, suppose we will have a continuing stream of players for whom we only know height, and need to predict their weights. Again, we will use the conditional mean to do so. For a player of height 72 inches, for example, our prediction might be

W = E(W | H = 72) (1.2)

Again, though, this is a population value, and all we have is sample data. How will we estimate E(W | H = 72) from that data?

First, some important notation: Recalling that μ is the traditional Greek letter to use for a population mean, let’s now use it to denote a function that gives us subpopulation means:

For any height t, define

    μ(t) = E(W | H = t) (1.3)

which is the mean weight of all people in the population who are of height t.

Since we can vary t, this is indeed a function, and it is known as the regression function of W on H.

So, μ(72.12) is the mean population weight of all players of height 72.12, μ(73.88) is the mean population weight of all players of height 73.88, and so on. These means are population values and thus unknown, but they do exist.

So, to predict the weight of a 71.6-inch tall player, we would use μ(71.6) — if we knew that value, which we don’t, since once again this is a population value while we only have sample data. So, we need to estimate that value from the (height, weight) pairs in our sample data, which we will denote by (H[1], W[1]), ... (H[1015], W[1015]). How might we do that? In the next two sections, we will explore ways to form our estimate, μ(t).

### 1.5.2 A First Estimator, Using a Nonparametric Approach

Our height data is only measured to the nearest inch, so instead of estimating values like μ(71.6), we’ll settle for μ(72) and so on. A very natural estimate for μ(72), again using the “hat” symbol to indicate “estimate of,” is the mean weight among all players in our sample for whom height is 72, i.e.

    μ(72) = mean of all W[i] such that H[i] = 72 (1.4) 

R’s tapply() can give us all the μ(t) at once:

```{r comment=NA}
#library(freqparcoord) 
#data(mlb) 
muhats <- tapply(mlb$Weight, mlb$Height, mean)
muhats 
```

In case you are not familiar with tapply(), here is what just happened. We asked R to partition the Weight variable into groups according to values of the Height variable, and then compute the mean weight in each group. So, the mean weight of people of height 72 in our sample was 190.3596. In other words, we would set μ(72) = 190.3596, μ(74) = 202.4566, and so on. (More detail on tapply() is given in the Code Complements section at the end of this chapter.)

Since we are simply performing the elementary statistics operation of estimating population means from samples, we can form confidence intervals (CIs). For this, we’ll need the “n” and sample standard deviation for each height group:

```{r comment=NA}
tapply(mlb$Weight, mlb$Height, length)
tapply(mlb$Weight, mlb$Height, sd)
```

An approximate 95% CI for μ(72), for example,

    190.3596 ± 1.96*(17.56349/sqrt(150))  (1.5)

or about (187.6, 193.2).

The above analysis takes what is called a nonparametric approach. To see why, let’s proceed to a parametric one, in the next section.

### 1.5.3 A Possibly Better Estimator, Using a Linear Model

All models are wrong, but some are useful — famed statistician George Box

So far, we have assumed nothing about the shape of μ(t) would have, if it were plotted on a graph. Again, it is unknown, but the function does exist, and thus it does correspond to some curve. But we might consider making an assumption on the shape of this unknown curve. That might seem odd, but you’ll see below that this is a very powerful, intuitively reasonable idea.

Toward this end, let’s plot those values of μ(t) we found above. We run 

```{r comment=NA}
par(pty = "s")
plot(67:83, muhats)
```

**Figure 1.1:** Plotted μ(t)

producing **Figure 1.1**.

Interestingly, the points in this plot seem to be near a straight line, suggesting that our unknown function μ(t) has a linear form, i.e. that

    μ(t) = c + dt (1.6) 

for some constants c and d, over the range of t appropriate to human heights. Or, in English,

    mean weight = c + d×height (1.7)

Don’t forget the word mean here! We are assuming that the mean weights in the various height subpopulations has the form (1.6), NOT that weight itself is this function of height, which can’t be true.

This is called a parametric model for μ(t), with parameters c and d. We will use this below to estimate μ(t).

Our earlier estimation approach, in Section 1.5.2, is called nonparametric. It is also called assumption-free, since it made no assumption at all about the shape of the μ(t) curve.

Note the following carefully:

- **Figure 1.1** suggests that our straight-line model for μ(t) may be less accurate at very small and very large values of t. This is hard to say, though, since we have rather few data points in those two regions, as seen in our earlier R calculations; there is only one person of height 83, for instance.

But again, in this chapter we are simply exploring, so let’s assume for now that the straight-line model for μ(t) is reasonably accurate. We will discuss in Chapter 6 how to assess the validity of this model.

- Since μ(t) is a population function, the constants c and d are population values, thus unknown. However, we can estimate them from our sample data. We do so using R’s lm() (“linear model”) function [4]:

```{r comment=NA}
lmout <- lm(Weight ~ Height, data = mlb)
lmout
```

This gives c = −151.133 and d = 4.783.

We would then set, for instance (using the caret instead of the hat, so as to distinguish from our previous estimator)

    μˇ(72) = −151.133 + 4.783 × 72 = 193.2666  (1.8) 

We need not type this expression into R by hand. Writing it in matrix-multiply form, it is

    (−151.133, 4.783) (1 72)'  (1.9)

```{r comment=NA}
B <- matrix(c(-151.133, 4.783), nrow = 1, ncol = 2)
B
X <- matrix(c(1, 72), nrow = 2, ncol = 1)
X
B %*% X
```

Be sure to see the need for that 1 in the second factor; it is used to multiply the -151.133.

Or, conveniently in R [5], we can exploit the fact that R’s coef() function fetches the coefficients c and d for us:

```{r comment=NA}
coef(lmout) %*% c(1, 72) 
```

We can form a confidence interval from this too. The standard error (Appendix ??) of μˇ(72) will be shown later to be obtainable using the R vcov() function:

```{r comment=NA}
tmp <- c (1, 72)
sqrt(tmp %*% vcov(lmout) %*% tmp)
193.2666 + 1.96*0.6859655 
193.2666 - 1.96*0.6859655 
```

(More detail on vcov() and coef() is presented in the Code Complements section at the end of this chapter.)

So, an approximate 95% CI for μ(72) under this model would be about (191.9, 194.6).

## 1.6 Parametric vs. Nonparametric Models

Now here is a major point: The CI we obtained from our linear model, (191.9, 194.6), was narrower than the nonparametric approach gave us, (187.6, 193.2); the former has width of about 2.7, while the latter’s is 5.6. In other words:

A parametric model is — if it is (approximately) valid — more powerful than the nonparametric one, yielding estimates of a regression function that tend to be more accurate than what the nonparametric approach gives us. This should translate to more accurate prediction as well.

Why should the linear model be more effective? Here is some intuition, say for estimating μ(72): As will be seen in Chapter 2, the lm() function uses all of the data to estimate the regression coefficients. In our case here, all 1015 data points played a role in the computation of μˇ(72), whereas only 150 of our observations were used in calculating our nonparametric estimate μ(72). The former, being based on much more data, should tend to be more accurate [6].

On the other hand, in some settings it may be difficult to find a valid parametric model, in which case a nonparametric approach may be much more effective. This interplay between parametric and nonparametric models will be a recurring theme in this book.

## 1.7 Several Predictor Variables

Now let’s predict weight from height and age. We first need some notation.

Say we are predicting a response variable Y from variables X(1), ..., X(k). The regression function is now defined to be

    μ(t[1], ..., t[k]) = E(Y | X(1) = t[1], ..., X(k) = t[k]) (1.10) 

In other words, μ(t[1], ..., t[k]) is the mean Y among all units (people, cars, whatever) in the population for which X(1) = t[1], ..., X(k) = t[k].

In our baseball data, Y, X(1) and X(2) might be weight, height and age, respectively. Then μ(72, 25) would be the population mean weight among all players of height 72 and age 25.

We will often use a vector notation

    μ(t) = E(Y | X = t)  (1.11) 

with t = (t[1], ..., t[k])′ and X = (X(1), ..., X(k))′, where ′ denotes matrix transpose [7].

### 1.7.1 Multi-predictor Linear Models

Let’s consider a parametric model for the baseball data,

    mean weight = c + d×height + e×age  (1.13)

#### 1.7.1.1 Estimation of Coefficients

We can again use lm() to obtain sample estimates of c, d and e:

```{r comment=NA}
lm(Weight ~ Height + Age, data = mlb) 
```

Note that the notation mlb$Weight ~ mlb$Height + mlb$Age simply means “predict weight from height and age.” The variable to be predicted is specified to the left of the tilde, and the predictor variables are written to the right of it. The + does not mean addition.

For example, d^ = 4.9236. Our estimated regression function is 

    μ(t1, t2) = −187.6382 + 4.9236*t1 + 0.9115*t2  (1.14)

where t1 and t2 are height and age, respectively. 

Setting t1 = 72 and t2 = 25, we find that

    μ(72, 25) = 189.6485  (1.15)

and we would predict the weight of a 72-inch tall, age 25 player to be about 190 pounds.

#### 1.7.1.2 The Description Goal

It was mentioned in Section 1.12.2 that regression analysis generally has one or both of two goals, Prediction and Description. In light of the latter, some brief comments on the magnitudes of the estimated coefficients would be useful at this point:

- We estimate that, on average (a key qualifier), each extra inch in height corresponds to almost 5 pounds of additional weight.

- We estimate that, on average, each extra year of age corresponds to almost a pound in extra weight.

That second item is an example of the Description goal in regression analysis, We may be interested in whether baseball players gain weight as they age, like “normal” people do. Athletes generally make great efforts to stay fit, but we may ask how well they succeed in this. The data here seem to indicate that baseball players indeed are prone to some degree of “weight creep” over time.

### 1.7.2 Nonparametric Regression Estimation: k-NN

Now let’s drop the linear model assumption (1.13), and estimate our regression function “from scratch,” as we did in Section 1.5.2. But here we will need to broaden our approach, as follows.

Again say we wish to estimate, using our data, the value of μ(72, 25). A potential problem is that there likely will not be any data points in our sample that exactly match those numbers, quite unlike the situation in (1.4), where μ(72) was based on 150 data points. Let’s check:

```{r comment=NA}
z <- mlb[mlb$Height == 72 & mlb$Age == 25, ] 
z
```

So, indeed there were no data points matching the 72 and 25 numbers. Since the ages are recorded to the nearest 0.01 year, this result is not surprising. But at any rate we thus we cannot set μ(72, 25) to be the mean weight among our sample data points satisfying those conditions, as we did in Section 1.5.2. And even if we had had a few data points of that nature, that would not have been enough to obtain an accurate estimate μ(72, 25).

Instead, what is done is use data points that are close to the desired prediction point. Again taking the [weight height age] case as a first example, this means that we would estimate μ(72, 25) by the average weight in our sample data among those data points for which height is near 72 and age is near 25.

### 1.7.3 Measures of Nearness

Nearness is generally defined as Euclidean distance:

    distance[(s[1], s[2], ..., s[k]), (t[1], t[2], ..., t[k])] = sqrt(((s[1] − t[1])^2 + ... + (s[k] − t[k])^2)  (1.16)

For instance, the distance from a player in our sample of height 72.5 and age 24.2 to the point (72, 25) would be

    sqrt((72.5 − 72)^2 + (24.2 − 25)^2) = 0.9434  (1.17)

The k-Nearest Neighbor (k-NN) method for estimating regression functions is simple: Find the k data points in our sample that are closest to the desired prediction point, and average their Y values.

### 1.7.4 The Code

Here is code to perform k-NN regression estimation:

```{r comment=NA}
# arguments:
# 
# xydata: matrix or data frame of full (X, Y) data, Y in last column
# regestpts: matrix or data frame of X vectors at which to estimate 
#            the regression function
# k: number of nearest neighbors 
# scalefirst: call scale() on the data first 
#
# value: estimated regression function at the given X values

knnest <- function(xydata, regestpts, k, scalefirst = FALSE) {
  require(FNN)
  if (is.vector(regestpts))
    regestpts <- matrix(regestpts, nrow = 1) 
  ycol <- ncol(xydata)
  x <- xydata[ , -ycol , drop = FALSE]
  if(scalefirst) {
    tmp <- rbind(x, regestpts) 
    tmp <- scale(tmp)
    x <- tmp[1:nrow(x), ]
    regestpts <- tmp[(nrow(x) + 1):nrow(tmp), ] 
  }
  colnames(regestpts) <- colnames(x) 
  y <- xydata[ , ycol]
  if(!is.matrix(regestpts))
    regestpts <- matrix(regestpts, nrow = 1)
  tmp <- get.knnx(data = x, query = regestpts, k = k) 
  idx <- tmp$nn.index
  meannear <- function(idxrow) mean(y[idxrow]) 
  apply(idx, 1, meannear)
}
```

Each row of regestpts is a point at which we wish to estimate the regression function. For example, let’s estimate μ(72,25), based on the 20 nearest neighbors at each point:

```{r comment=NA}
#knnest(mlb[ , c(4, 6, 5)], c(72, 25), 20, scalefirst = TRUE) # gives an error
# try scaling xydata and regestpts seperately, so don't change the data structures
# calculate mean and sd vectors of xydata (with or without regestpts?)
# then apply to mean and regestpts seperately to scale
# seems to work with "scalefirst = TRUE" in Ch. 3, however - why?
knnest(mlb[ , c(4, 6, 5)], c(72, 25), 20, scalefirst = FALSE) 
```

So we would predict the weight of a 72-inches tall, age 25 player to be about 189 pounds, not much different — in this instance — from what we obtained earlier with the linear model.

The call to the built-in R function scale() is useful if our predictor variables are of widely different magnitudes. In such a setting, the larger-magnitude variables are in essence being given heavier weightings in the distance computation. However, rerunning the above analysis without scaling (not shown) produces the same result.

## 1.8 After Fitting a Model, How Do We Use It for Prediction?

As noted, our goal in regression analysis could be either Prediction or Description (or both). How specifically does the former case work?

### 1.8.1 Parametric Settings

The parametric case is the simpler one. We fit our data, write down the result, and then use that result in the future whenever we are called upon to do a prediction.

Recall Section 1.7.1.1. It was mentioned there that in that setting, we probably are not interested in the Prediction goal, but just as an illustration, suppose we do wish to predict. We fit our model to our data — called our training data — resulting in our estimated regression function, (1.14). From now on, whenever we need to predict a player’s weight, given his height and age, we simply plug those values into (1.14).

### 1.8.2 Nonparametric Settings

The nonparametric case is a little more involved, because we have no explicit equation like (1.14). Nevertheless, we use our training data in the same way.

For instance, say we need to predict the weight of a player whose height and age are 73.2 and 26.5, respectively. Our predicted value will be then μ^(73.2, 26.5). To obtain that, we go back to our training data, find the k nearest points to (73.2, 25.5), and average the weights of those k players. We would go through this process each time we are called upon to perform a prediction.

A variation:

A slightly different approach, which we will use here, is as follows. Denote our training set data as (X[1], Y[1]), ..., (X[n], Y[m]), where again the X[i] are typically vectors, e.g. (height, age). We estimate our regression function at each of the points Xi, forming μ^(X[i]), i = 1, ..., n. Then, when faced with a new case (X, Y) for which Y is unknown, we find the single closest X[i] to X, and guess Y to be 1 or 0, depending on whether μ^(X[i]) > 0.5.

## 1.9 Overfitting, Bias and Variance

One major concern in model development is overfitting, meaning to fit such an elaborate model that it “captures the noise rather than the signal.” This vague description is not satisfactory, and it will be discussed in a precise manner in Chapter 9. But for now the point is that, after fitting our model, we are concerned that it may fit our training data well but not predict well on new data in the future [8].

### 1.9.1 Intuition

To see how overfitting may occur, consider the famous bias-variance tradeoff, illustrated in the following example. Again, keep in mind that the treatment will at this point just be intuitive, not mathematical.

Long ago, when I was just finishing my doctoral study, I had my first experience in statistical consulting. A chain of hospitals was interested in comparing the levels of quality of care given to heart attack patients at its various locations. A problem was noticed by the chain regarding straight comparison of raw survival rates: One of the locations served a largely elderly population, and since this demographic presumably has more difficulty surviving a heart attack, this particular hospital may misleadingly appear to be giving inferior care.

An analyst who may not realize the age issue here would thus be biasing the results. The term “bias” here doesn’t mean deliberate distortion of the analysis, just that one is using a less accurate model then one should, actually “skewed” in the common vernacular. And it is permanent bias, in the sense that it won’t disappear, no matter how large a sample we take. Accordingly, by adding more predictor variables in a regression model, we are reducing bias.

Or, suppose we use a regression model which is linear in our predictors, but the true regression function is nonlinear. This is bias too, and again it won’t go away even if we make the sample size huge.

On the other hand, we must keep in mind that our data consists is a sample from a population. In the hospital example, for instance, the patients on which we have data can be considered a sample from the (somewhat conceptual) population of all patients at this hospital, past, present and future. A different sample would produce different regression coefficient estimates. In other words, there is variability in those coefficients from one sample to another, i.e. variance. We hope that that variance is small, which gives us confidence that the sample we have is representative.

But the more predictor variables we have, the more variability there is in the inputs to our regression calculations, and thus the larger the variances of the estimated coefficients [9].

In other words:

In deciding how many (and which) predictors to use, we have a tradeoff. The richer our model, the less bias, but the more variance.

The trick is somehow to find a “happy medium,” easier said than done. Chapter 9 will cover this in depth, but for now, we introduce a common method for approaching the problem:

### 1.9.2 Rough Rule of Thumb

The issue of how many predictors to use to simultaneously avoid overfitting and still produce a good model is nuanced, and in fact this is still not fully resolved. Chapter 9 will be devoted to this complex matter.

Until then, though it is worth using the following:

Rough Rule of Thumb (Tukey): For a data set consisting of n observations, use fewer than sqrt(n) predictors.

### 1.9.3 Cross-Validation

Toward that end, it is common to artificially create a set of “new” data and try things out. Instead of using all of our collected data as our training set, we set aside part of it to serve as simulated “new” data. This is called the validation set or test set. The remainder will be our actual training data. In other words, we randomly partition our original data, taking one part as our training set and the other part to play the role of new data. We fit our model, or models, to the training set, then do prediction on the test set, pretending its response variable values are unknown. We then compare to the real values. This will give us an idea of how well our models will predict in the future. This is called cross-validation.

The above description is a little vague, and since there is nothing like code to clarify the meaning of an algorithm, let’s develop some. Here first is code to do the random partitioning of data, with a proportion p to go to the training set:

```{r comment=NA}
xvalpart <- function(data, p) { 
  n <- nrow(data)
  ntrain <- round(p*n)
  trainidxs <- sample(1:n, ntrain, replace = FALSE) 
  valididxs <- setdiff(1:n, trainidxs) 
  list(train = data[trainidxs, ], valid = data[valididxs, ])
}
```

Now to perform cross-validation, we’ll consider the parametric and nonparametric cases separately, in the next two sections.

### 1.9.4 Linear Model Case

To do cross-validation for linear models, we could use this code [10]. 

#### 1.9.4.1 The Code

```{r comment=NA}
# arguments:
# 
# data: full data
# ycol: column number of response variable 
# predvars: column numbers of predictors
# p: proportion for training set 
# meanabs: see ’value’ below

# value: if meanabs is TRUE, the mean absolute 
#        prediction error; otherwise, an R list 
#        containing prediction, real Y

xvallm <- function(data, ycol, predvars, p, meanabs = TRUE) { 
  tmp <- xvalpart(data, p)
  train <- tmp$train
  valid <- tmp$valid
  # fit model to training data
  trainy <- train [ , ycol]
  trainpreds <- train [ , predvars]
  # we’ll be using matrices , e.g. in lm()
  trainpreds <- as.matrix(trainpreds)
  lmout <- lm(trainy ~ trainpreds)
  # apply fitted model to validation data
  validpreds <- as.matrix(valid[,predvars]) 
  predy <- cbind(1,validpreds) %*% coef(lmout) 
  realy <- valid [ , ycol]
  if (meanabs) return(mean(abs(predy - realy))) 
  list(predy = predy, realy = realy)
}
```

#### 1.9.4.2 Matrix Partitioning

Note that in the line

    predy <− cbind(1, validpreds) %*% coef(lmout)

we have exploited the same matrix multiplication property as in (1.9). Here, though, we have applied it at the matrix level. Such operations will become common in some parts of this book, so a brief digression will be worthwhile. For a concrete numerical example, consider the vector

    |(−1, 2)(3, 8)′| = |13|
    |( 2, 5)(3, 8)′|   |46|  (1.18)

```{r comment=NA}
B11 <- matrix(c(-1, 2), nrow = 1, ncol = 2)
B11
B12 <- matrix(c( 3, 8), nrow = 2, ncol = 1)
B12
B1 <- B11 %*% B12
B1

B21 <- matrix(c( 2, 5), nrow = 1, ncol = 2)
B21
B22 <- matrix(c( 3, 8), nrow = 2, ncol = 1)
B22
B2 <- B21 %*% B22
B2

X <- matrix(c(B1, B2), nrow = 2, ncol = 1)
X
```

The reader should verify that “distributing out” that common (3, 8)′ factor is valid algebra:

    |−1 2|*|3| = |13|
    | 2 5| |8|   |46|  (1.19)

```{r comment=NA}
B <- matrix(c(-1, 2, 2, 5), nrow = 2, ncol = 2)
B
X <- matrix(c(3, 8), nrow = 2, ncol = 1)
X
B %*% X
```

#### 1.9.4.3 Applying the Code

Let’s try cross-validation on the [weight height age] data, using mean absolute prediction error as our criterion for prediction accuracy:

```{r comment=NA}
xvallm(mlb, 5 , c(4, 6), 2/3)
# 12.94553 in book - need set.seed() to reproduce?
```


So, on average we would be off by about 13 pounds. We might improve upon this by using the data’s Position variable, but we’ll leave that for later.

### 1.9.5 k-NN Case

Here is the code for performing cross-validation for k-NN:

```{r comment=NA}
# arguments:
# 
# data: full data
# ycol: column number of response variable 
# predvars  column numbers of predictors 
# k: number of nearest neighbors
# p: proportion for training set
# meanabs : see ’ value ’ below

# value: if meanabs is TRUE, the mean absolute 
#        prediction error; otherwise, an R list 
#        containing prediction, real Y

xvalknn <- function(data, ycol, predvars, k, p, meanabs = TRUE) {
  tmp <- xvalpart(data, p)
  train <- tmp$train
  valid <- tmp$valid
  trainxy <- data[ , c(predvars, ycol)] 
  validx <- valid[ , predvars]
  validx <- as.matrix(validx)
  predy <- knnest(trainxy, validx, k)
  realy <- valid [ , ycol]
  if (meanabs) return(mean(abs(predy - realy ))) 
  list(predy = predy, realy = realy)
}
```

So, how well does k-NN predict?

```{r comment=NA}
xvallm(mlb, 5, c(4, 6), 2/3) 
# 12.94553 in book - need set.seed() to reproduce?
```

The two methods gave similar results. However, this depended on choosing a value of 20 for k, the number of nearest neighbors. We could have tried other values of k, and in fact could have used cross-validation to choose the “best” value.

### 1.9.6 Choosing the Partition Sizes

One other problem, of course, is that we did have a random partition of our data. A different one might have given substantially different results.

In addition, there is the matter of choosing the sizes of the training and validation sets (e.g. via the argument p in xvalpart()). We have a classical tradeoff at work here: Let k be the size of our training set. If we make k too large, the validation set will be too small for an accurate measure of prediction accuracy. We won’t have that problem if we set k to a smaller size, but then we are measuring the predictive ability of only k observations, whereas in the end we will be using all n observations for predicting new data.

The Leaving One-Out Method solves this problem, albeit at the expense of much more computation. It will be presented in Section 2.7.5.

## 1.10 Example: Bike-Sharing Data

We now return to the bike-sharing data. Our little excursion to the simpler data set, involving baseball player weights and heights, helped introduce the concepts in a less complex setting. The bike-sharing data set is more complicated in several ways:

- Complication (a): It has more potential predictor variables.

- Complication (b): It includes some nominal variables, such as Day of Week. The latter is technically numeric, 0 through 6, but those codes are just names [11]. There is no reason, for instance, that Sunday, Thursday and Friday should have an ordinal relation in terms of ridership just because 0 < 4 < 5.

- Complication (c): It has some potentially nonlinear relations. For instance, people don’t like to ride bikes in freezing weather, but they are not keen on riding on really hot days either. Thus we might suspect that the relation of ridership to temperature rises at first, eventually reaching a peak, but declines somewhat as the temperature increases further.

Now that we know some of the basic issues from analyzing the baseball data, we can treat this more complicated data set.

Let’s read in the bike-sharing data. We’ll restrict attention to the first year [12], and since we will focus on the registered riders, let’s shorten the name for convenience:

```{r comment=NA}
# need to download "day.csv" and store in working directory
setwd("~/GitHub/R")
shar <- read.csv("day.csv", header = TRUE) 
shar <- shar[1:365, ]
names(shar)[15] <- "reg"
```

### 1.10.1 Linear Modeling of μ(t)

In view of Complication (c) above, the inclusion of the word linear in the title of our current section might seem contradictory. But one must look carefully at what is linear or not, and we will see shortly that, yes, we can use linear models to analyze nonlinear relations.

Let’s first check whether the ridership-temperature relation seems nonlinear, as we have speculated:

```{r comment=NA}
par(pty = "s")
plot(shar$temp, shar$reg) 
```

**Figure 1.2:** Ridership vs. Temperature

The result is shown in **Figure 1.2**.

There seem to be some interesting groupings among the data, likely due to the other variables, but putting those aside for now, the plot does seem to suggest that ridership is somewhat associated with temperature in the “first up, then later down” form as we had guessed.

Thus a linear model of the form

    mean ridership = c + d×temperature (1.20) 

would seem inappropriate. But don’t give up so quickly! A model like

    mean ridership = c + d×temperature + e×temperature^2 (1.21)

i.e., with a temperature-squared term added, might work fine. A negative value for e would give us the “first up, then later down” behavior we want our model to have.

And there is good news — the model (1.21) is actually linear! We say that the expression is linear in the parameters, even though it is nonlinear with respect to the temperature variable. This means that if we multiply each of c, d and e by, say, 8, then the values of the left and right sides of the equation both increase eightfold.

Another way to see this is that in calling lm(), we can simply regard squared temperature as a new variable:

```{r comment=NA}
shar$temp2 <- shar$temp^2
lm(reg ~ temp + temp2, data = shar)
```

And note that, sure enough, the coefficient of the squared term, e^ = −11756, did indeed turn out to be negative.

Of course, we want to predict from many variables, not just temperature, so let’s now turn to Complication (b) cited earlier, the presence of nominal data. This is not much of a problem either.

Such situations are generally handled by setting up what are called indicator variables or dummy variables. The former term alludes to the fact that our variable will indicate whether a certain condition holds or not, with 1 coding the yes case and 0 indicating no.

We could, for instance, set up such a variable for Tuesday data: 

```{r comment=NA}
shar$tues <- as.integer(shar$weekday == 2)
```

Indeed, we could define six variables like this, one for each of the days Monday through Saturday. Note that Sunday would then be indicated indirectly, via the other variables all having the value 0. A direct Sunday variable would be redundant, and in fact would present mathematical problems, as we’ll see in Chapter 8.

However, let’s opt for a simpler analysis, in which we distinguish only between weekend days and week days, i.e. define a dummy variable that is 1 for Monday through Friday, and 0 for the other days. Actually, those who assembled the data set already defined such a variable, which they named workingday [13].

We incorporate this into our linear model:

    mean reg = c + d×temp + e×temp^2 + f×workingday (1.22) 

There are several other dummy variables that we could add to our model, but for this introductory example let’s define just one more:

```{r comment=NA}
shar$clearday <- as.integer(shar$weathersit == 1)
```

So, our regression model will be

    mean reg = β0 + β1*temp + β2*temp^2 + β3*workingday + β4*clearday  (1.23)

As is traditional, here we have used subscripted versions of the Greek letter β to denote our equation coefficients, rather than c, d and so on.

So, let’s run this through lm():

```{r comment=NA}
#lmout <- lm(reg ~ temp + temp2 + workingday + clearday, data = shar[test, ]) 
# 'test' not defined, so remove
lmout <- lm(reg ~ temp + temp2 + workingday + clearday, data = shar)
```

(The use of the data argument saved typing of the data set name shar and clutter.)

The return value of lm(), assigned here to lmout, is a very complicated R object, of class ”lm”. We shouldn’t inspect it in detail now, but let’s at least print the object, which in R’s interactive mode can be done simply by typing the name, which automatically calls print() on the object [14]:

```{r comment=NA}
lmout
```

Remember, the population function μ(t) is unknown, so the βi are unknown. The above coefficients are merely sample-based estimates. For example, using our usual “hat” notation to mean “estimate of,” we have that

    β3 = 988.5 (1.24)

In other words, estimated regression function is

    μ^(t[1], t[2], t[3], t[4]) = −2310.3 + 17342.2*t[1] − 13434.7*t[2] + 988.5*t[3] + 760.3*t[4]  (1.25)

where t[2] = t[1]^2.

So, what should we predict for number of riders on the type of day described at the outset of this chapter — Sunday, sunny, 62 degrees Fahrenheit? First, note that the temp variable is scaled to [0, 1], as

    (Celsius temperature − minimum)/(maximum - minimum) (1.26)

where the minimum and maximum here were -8 and 39, respectively. A Fahrenheit temperature of 62 degrees corresponds to a scaled value of 0.525. So, our predicted number of riders is

    −2310.3 + 17342.2×0.525 − 13434.7×0.5252 + 988.5×0 + 760.3×1 (1.27)

which as before we can conveniently evaluate as

```{r comment=NA}
coef(lmout) %*% c(1, 0.525, 0.525^2, 0, 1)
```

So, our predicted number of riders for sunny, 62-degree Sundays will be about 3852.

One can also form confidence intervals and perform significance tests on the βi. We’ll go into this in Chapter 2, but some brief comments on the magnitudes and signs of the βi^ is useful at this point:

- As noted, the estimated coefficient of temp2 is negative, consistent with our intuition. Note, though, that it is actually more negative than when we predicted reg from only temperature and its square. This is typical, and will be discussed in detail in Chapter 7.

- The estimated coefficient for workingday is positive. This too matches our intuition, as presumably many of the registered riders use the bikes to commute to work. The value of the estimate here, 988.5, indicates that, for fixed temperature and weather conditions, weekdays tend to have close to 1000 more registered riders than weekends.

- Similarly, the coefficient of clearday suggests that for fixed temperature and day of the week, there are about 760 more riders on clear days than on other days.

### 1.10.2 Nonparametric Analysis

Let’s see what k-NN gives us as our predicted value for sunny, 62-degree Sundays, say with values of 20 and 50 for k:

```{r comment=NA}
knnest(shar[ , c(10, 8, 17, 15)], matrix(c(0.525, 0, 1), nrow = 1), 20)
knnest(shar[ , c(10, 8, 17, 15)], matrix(c(0.525, 0, 1), nrow = 1), 10)
```

This is quite different from what the linear model gave us. Let’s see how the two approaches compare in cross-validation:

```{r comment=NA}
xvallm(shar, 15, c(10, 18, 8, 17), 2/3) 
xvalknn(shar, 15, c(10, 8, 17), 20, 2/3) 
xvalknn(shar, 15, c(10, 8, 17), 10, 2/3) 
```

The nonparametric approach did substantially better, possibly indicating that our linear model was not valid. Of course, there still is the problems of not knowing what value to use for k, the fact that our partition was random and so on. These issues will be discussed in detail in succeeding chapters.

## 1.11 Interaction Terms

Let’s take another look at (1.23), specifically the term involving the variable workingday, a dummy indicating a nonholiday Monday through Friday. Our estimate for β3 turned out to be 988.5, meaning that, holding temperature and the other variables fixed, there are 988.5 additional riders on workingdays.

But implicit in this model is that the workingday effect is the same on low-temprerature days as on warmer days. For a broader model that does not make this assumption, we could add an interaction term, consisting of a product of workingday and temp:

    mean reg = β0 + β1*temp + β2*temp^2  
               + β3*workingday + β4*clearday (1.28) 
               + β5*temp×workingday  (1.29)

How does this model work? Let’s illustrate it with a new data set.

### 1.11.1 Example: Salaries of Female Programmers and Engineers

This data is from the 2000 U.S. Census, consisting of 20,090 programmers and engineers in the Silicon Valley area. The data set is included in the freqparcoord package on CRAN. Suppose we are working toward a Description goal, specifically the effects of gender on wage income.

As with our bike-sharing data, we’ll add a quadratic term, in this case on the age variable, reflecting the fact that many older programmers and engineers encounter trouble finding work after age 35 or so. Let’s restrict our analysis to workers having at least a Bachelor’s degree, and look at the variables age, age2, sex (coded 1 for male, 2 for female), wkswrked (number of weeks worked), ms, phd and wageinc:

```{r comment=NA}
#library(freqparcoord)
data(prgeng)
prgeng$age2 <- prgeng$age^2
edu <- prgeng$educ
prgeng$ms  <- as.integer(edu == 14) 
prgeng$phd <- as.integer(edu == 16)
prgeng$fem <- prgeng$sex - 1
tmp <- prgeng[edu >= 13, ]
pe <- tmp[ , c(1, 12, 9, 13, 14, 15, 8)] 
pe <- as.matrix(pe)
```

Our model is

    mean wageinc = β0 + β1*age + β2*age^2 + β3*wkswrkd + β4*ms + β5*phd + β6*fem  (1.30)

We find the following:

```{r comment=NA}
summary(lm(pe[ , 7] ~ pe[ , -7]))
```

The results are striking in terms of gender: With age, education and so on held constant, women are estimated to have incomes about $11,177 lower than comparable men.

But this analysis implicitly assumes that the female wage deficit is, for instance, uniform across educational levels. To see this, consider (1.30). Being female makes a β6 difference, no matter what the values of ms and phd are. To generalize our model in this regard, let’s define two interaction variables [15]:

```{r comment=NA}
msfem  <- pe[ , 4]*pe[ , 6]
phdfem <- pe[ , 5]*pe[ , 6]
pe <- cbind(pe, msfem, phdfem)
```

Our model is now

    mean wageinc = β0 + β1*age + β2*age^2 + β3*wkswrkd + β4*ms + β5*phd + β6*fem + β7*msfem + β8*phdfem  (1.31) 

So, now instead of there being a single number for the “female effect,” β6, we how have two:

- Female effect for Master’s degree holders: β6 + β7

- Female effect for PhD degree holders β6 + β8 

So, let’s rerun the regression analysis:

```{r comment=NA}
summary(lm(pe[ , 7] ~ pe[ , -7]))
```

The estimated values of the two female effects are -9091.230 - 5088.779 = -14180.01, and 9091.230 - 14831.582 = -23922.81. A few points jump out here:

- Once one factors in educational level, the gender gap is seen to be even worse than before.

- The gap is worse at the PhD level than the Master’s, likely because of the generally higher wages for the latter.

Thus we still have many questions to answer, especially since we haven’t consider other types of interactions yet. This story is not over yet, and will be pursued in detail in Chapter 7.

## 1.12 Classification Techniques

Recall the hospital example in Section 1.9.1. There the response variable is nominal, represented by a dummy variable taking the values 1 and 0, depending on whether the patient survives or not. This is referred to as a classification problem, because we are trying to predict which class the population unit belongs to — in this case, whether the patient will belong to the survival or non-survival class. We could set up dummy variables for each of the hospital branches, and use these to assess whether some were doing a better job than others, while correcting for variations in age distribution from one branch to another. (Thus our goal here is Description rather than directly Prediction itself.)

This will be explained in detail in Chapter 4, but the point is that we are predicting a 1-0 variable. In a marketing context, we might be predicting which customers are more likely to purchase a certain product. In a computer vision context, we may want to predict whether an image contains a certain object. In the future, if we are fortunate enough to develop relevant data, we might even try our hand at predicting earthquakes.

Classification applications are extremely common. And in many cases there are more than two classes, such as in identifying many different printed characters in computer vision.

In a number of applications, it is desirable to actually convert a problem with a numeric response variable into a classification problem. For instance, there may be some legal or contractual aspect that comes into play when our variable V is above a certain level c, and we are only interested in whether the requirement is satisfied. We could replace V with a new variable

    Y = 1, if V > c 
      = 0, if V ≤ c (1.32)

Classification methods will play a major role in this book.

### 1.12.1 It’s a Regression Problem!

Recall that the regression function is the conditional mean: 

    μ(t) = E(Y | X = t) (1.33)

(As usual, X and t may be vector-valued.) In the classification case, Y is an indicator variable, so from Appendix ??, we know its mean is the probability that Y = 1. In other words,

    μ(t) = P(Y = 1 | X = t)  (1.34) 

The great implication of this is that the extensive knowledge about regression analysis developed over the years can be applied to the classification problem.

An intuitive strategy — but, as we’ll see, NOT the only appropriate one — would be to guess that Y = 1 if the conditional probability of 1 is greater than 0.5, and guess 0 otherwise. In other words,

    guess for Y = 1, if μ(X) > 0.5 
                = 0, if μ(X) ≤ 0.5  (1.35)

It turns out that this strategy is optimal, in that it minimizes the overall misclassification error rate (see Section 1.13.2 in the Mathematical Complements portion of this chapter). However, it should be noted that this is not the only possible criterion that might be used. We’ll return to this issue in Chapter 5.

As before, note that (1.34) is a population quantity. We’ll need to estimate it from our sample data.

### 1.12.2 Example: Bike-Sharing Data

Let’s take as our example the situation in which ridership is above 3,500 bikes, which we will call HighUsage:

```{r comment=NA}
shar$highuse <- as.integer(shar$reg > 3500)
```

We’ll try to predict that variable. Let’s again use our earlier example, of a Sunday, clear weather, 62 degrees. Should we guess that this will be a High Usage day?

We can use our k-NN approach just as before:

```{r comment=NA}
knnest(shar[ , c(10, 8, 18, 19)], c(0.525, 0, 1), 20) 
```

We estimate that there is a 10% chance of that day having HighUsage. 

The parametric case is a little more involved. A model like

    probability of HighUsage = β0 + β1*temp + β2*temp^2 + β3*workingday + β4*clearday  (1.36)

could be used, but would not be very satisfying. The left-hand side of (1.36), as a probability, should be in [0,1], but the right-hand side could in principle fall far outside that range.

Instead, the most common model for conditional probability is logistic regression:

    probability of HighUsage = l(β0 + β1*temp + β2*temp^2 + β3*workingday + β4*clearday)  (1.37)

**Figure 1.3:** Logistic Function

where l(s) is the logistic function, 

    l(s) = 1/(1 + exp(−s))  (1.38)

Our model, then is

    μ(t[1], t[2], t[3], t[4]) = 1/(1 + exp(−(β0 + β1*t1 + β2*t2 + β3*t3 + β4*t4))  (1.39) 

where t[1] is temperature, t[2] is the square of temperature, and so on. We wish to estimate μ(62, 622, 0, 1).

Note the form of the curve, shown in **Figure 1.3**. The appeal of this model is clear at a glance: First, the logistic function produces a value in [0,1], as appropriate for modeling a probability. Second, it is a monotone increasing function in each of the variables in (1.37), just as was the case in (1.23) for predicting our numeric variable, reg. Other motivations for using the logistic model will be discussed in Chapter 4.

R provides the glm() (“generalized linear model”) function for several nonlinear model families, including the logistic [16], which is designated via family = binomial:

```{r comment=NA}
glmout <- glm(highuse ~ temp + temp2 + workingday + clearday, data = shar, family = binomial)
glmout 
tmp <- coef(glmout) %*% c(1, 0.525, 0.525^2, 0, 1) 
1/(1 + exp(-tmp))
```

So, our parametric model gives an almost identical result here to the one arising from k-NN, about a 10% probability of HighUsage.

We can perform cross-validation too, and will do so in later chapters. For now, note that our accuracy criterion should change, say to the proportion of misclassified data points.

## 1.13 Mathematical Complements

Certain claims of optimality were made in this chapter. Let’s prove them.

### 1.13.1 μ(t) Minimizes Mean Squared Prediction Error

Claim: Consider all the functions f() with which we might predict Y from X, i.e., Y^ = f(X). The one that minimizes mean squared prediction error, E[(Y − f(X))^2], is the regression function, μ(t) = E(Y | X = t).

(Note that the above involves population quantities, not samples. Consider the quantity E[(Y − f(X))^2], for instance. It is the mean squared prediction over all (X, Y) pairs in the population.)

To derive this, first ask, for any (finite-variance) random variable W, what number c that minimizes the quantity E[(W  − c)^2]? The answer is c = EW. To see this, write

    E[(W − c)^2] = E(W^2 − 2cW + c^2] = E(W^2) − 2cEW + c2  (1.40)

Setting to 0 the derivative of the right-hand side with respect to c, we find that indeed, c = EW.

Now to show the original claim, use iterated expectation (Appendix ??) to write

    E[(Y − f(X))^2] = E[E((Y − f(X))^2 | X)]  (1.41)

In the inner expectation, X is a constant, and from the above we know that the minimizing value of f(X) is “EW,” in this case E(Y | X), i.e. μ(X). Since that minimizes the inner expectation for any X, the overall expectation is minimized too.

### 1.13.2 μ(t) Minimizes the Misclassification Rate

This result concerns the classification context. It shows that if we know the population distribution — we don’t, but are going through this exercise to guide our intuition — the conditional mean provides the optimal action in the classification context.

Remember, in this context, μ(t) = P (Y | X = t), i.e. the conditional mean reduces to the conditional probability. Now plug in X for t, and we have the following.

Claim: Consider all rules based on X that produce a guess Y^, taking on values 0 and 1. The one that minimizes the overall misclassification rate P (Y^ != Y) is

    Y^ = 1, if μ(X) > 0.5
       = 0, if μ(X) ≤ 0.5  (1.42)

The claim is completely intuitive, almost trivial: After observing X, how should we guess Y? If conditionally Y has a greater than 50% chance of being 1, then guess it to be 1!

(Note: In some settings, a “false positive” may be worse than a “false negative,” or vice versa. The reader should ponder how to modify the material here for such a situation. We’ll return to this issue in Chapter 5.)

Think of this simple situation: There is a biased coin, with known prob- ability of heads p. The coin will be tossed once, and we are supposed to guess the outcome.

Let’s name your guess g (a nonrandom constant), and let C denote the as-yet-unknown outcome of the toss (1 for heads, 0 for tails). Then the reader should check that, no matter whether we choose 0 or 1 for g, the probability that we guess correctly is

    P(C = g) = P(C = 1)g + P(C = 0)(1 − g)  (1.43)
             = pg + (1 − p)(1 − g)          (1.44) 
             = [2p − 1]g + 1 − p            (1.45)

Now remember, p is known. How should we choose g, 0 or 1, in order to maximize (1.45), the probability that our guess is correct? Inspecting (1.45) shows that maximizing that expression will depend on whether 2p − 1 is positive or negative, i.e., whether p > 0.5 or not. In the former case we should choose g = 1, while in the latter case g should be chosen to be 0.

The above reasoning gives us the very intuitive — actually trivial, when expressed in English — result:

If the coin is biased toward heads, we should guess heads. If the coin is biased toward tails, we should guess tails.

Now returning to our original claim, write

    P(Y^ = Y) = E[P(Y^ = Y |X)]  (1.46)

In that inner probability, “p” is

    P(Y = 1 | X) = μ(X)  (1.47)

which completes the proof.

## 1.14 Code Complements

### 1.14.1 The Functions tapply() and Its Cousins

In Section 1.5.2 we had occasion to use R’s tapply(), a hihgly useful feature of the language. To explain it, let’s start with useful function, split().

Consider this tiny data frame:

```{r comment=NA, echo=FALSE}
gender <- c("m", "f", "m", "f", "f")
height <- c(66, 67, 72, 63, 63)
x <- data.frame(gender = gender, height = height)
```

```{r comment=NA}
x
```

Now let’s split by gender:

```{r comment=NA}
xs <- split(x, x$gender) 
xs
```

Note the types of the objects: 

- xs is an R list

- xs\$f and xs\$m are data frames, the male and female subsets of x 

We could then find the mean heights in each gender:

```{r comment=NA}
mean(xs$f$height) 
mean(xs$m$height)
```

But with tapply(), we can combine the two operations:

```{r comment=NA}
tapply(x$height, x$gender, mean)
```

The first argument of tapply() must be a vector, but the function that is applied can be vector-valued. Say we want to find not only the mean but also the standard deviation. We can do this:

```{r comment=NA}
tapply(x$height, x$gender, function(w) c(mean(w), sd(w))) 
```

Here, our function (which we defined “on the spot,” within our call to tapply(), produces a vector of two components. We asked tapply() to call that function on our vector of heights, doing so separately for each gender.

## 1.15 Function Dispatch

The return value from a call to lm() is an object of R’s S3 class structure; the class, not surprisingly, is named ”lm”. It turns out that the functions coef() and vcov() mentioned in this chapter are actually related to this class, as follows.

Recall our usage, on the baseball player data:

```{r comment=NA}
lmout <- lm(Weight ~ Height, data = mlb)
```

The call to coef extracted the vector of estimated regression coefficents (which we also could have obtained as lmout$coefficents). But here is what happened behind the scenes:

The R function coef() is a generic function, which means it’s just a placeholder, not a “real” function. When we call it, the R interpreter says,

This is a generic function, so I need to relay this call to the one associated with this class, ”lm”. That means I need to check whether we have a function coef.lm(). Oh, yes we do, so let’s call that.

That relaying action is referred to in R terminology as the original call being dispatched to coef.lm().

This is a nice convenience. Consider another generic R function, plot(). No matter what object we are working with, the odds are that some kind of plotting function has been written for it. We can just call plot() on the given object, and leave it to R to find the proper call. (This includes the ”lm” class; try it on our lmout above!)

Similarly, there are a number of R classes on which coef() is defined, and the same is true for vcov().

Exercises

1. Consider the bodyfat data mentioned in Section 1.2. Use lm() to form a prediction equation for density from the other variables (skipping the first three), and comment on whether use of indirect methods in this way seems feasible.

2. Suppose the joint density of (X,Y) is 3\*s^2\*exp(−s\*t), 1 < s < 2, 0 < t < −∞. Find the regression function μ(s) = E(Y | X = s).

3. For (X, Y) in the notation of Section 1.13.1, show that the predicted value μ(X) and the prediction error Y − μ(X) are uncorrelated.

4. Suppose X is a scalar random variable with density g. We are interested in the nearest neighbors to a point t, based on a random sample X[1], ..., X[n] from g. Find Lk denote the cumulative distribution function of the distance of the kth-nearest neighbor to t.

[1] Available at https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset.

[2] This is a somewhat slippery notion, because there may be systemic differences from the present and the distant past and distant future, but let’s suppose we’ve resolved that by limiting our time range.

[3] Note that the “hat” notation ˆ is the traditional one for “estimate of.”

[4] Details on how the estimation is done will be given in Chapter 2.

[5] In order to gain a more solid understanding of the concepts, we will refrain from using R’s predict() function for now. It will be introduced later, though, in Section 4.4.4.

[6] Note the phrase tend to here. As you know, in statistics one usually cannot say that one estimator is always better than another, because anomalous samples do have some nonzero probability of occurring.

[7] Our vectors in this book are column vectors. However, since they occupy a lot of space on a page, we will often show them as transposes of rows. For instance, we will often write (5, 12, 13)′ instead of
| 5|
|12|   
|13| (1.12)

[8] Note that this assumes that nothing changes in the system under study between the time we collect our training data and the time we do future predictions.

[9] I wish to thank Ariel Shin for this interpretation.

[10] There are sophisticated packages on CRAN for this, such as cvTools. But to keep things simple, and to better understand the concepts, we will write our own code. Similarly, as mentioned, we will not use R’s predict() function for the time being.

[11] Hence the term nominal.

[12] There appears to have been some systemic change in the second year, and while this could be modeled, we’ll keep things simple by considering only the first year.

[13] More specifically, a value of 1 for this variable indicates that the day is in the Monday-Friday range and it is not a holiday.

[14] If you know about dispatch in R, invoking print() will cause a class-specific function to be run, in this case print.lm().

[15] Rather than creating the interaction terms “manually” as is done here, one can use R colon operator, which automates the process. This is not done here, so as to ensure that the reader fully understands the meaning of interaction terms. For information on the colon operator, type ?formula at the R prompt.

[16] Often called “logit,” by the way.
