---
title: "draftregclass04"
author: "Robert A. Stevens"
date: "September 2, 2016"
output: html_document
---

*From Linear Models to Machine Learning: Regression and Classification, with R Examples*

by Norman Matloff

# Chapter 4: Nonlinear Models

Consider our bike-sharing data (e.g., Section 1.12.2). Suppose we have several years of data. On the assumption that ridership trends are seasonal, and that there is no other time trend (e.g. no long-term growth in the program), then there would be a periodic relation between ridership R and G, the day in our data; here G would take the values 1, 2, 3, ..., with the top value being, say, 3×365 = 1095 for three consecutive years of data [1]. Assuming that we have no other predictors, we might try fitting the model with a sine term:

    mean R = β0 + β1*sin(2π·G/365)  (4.1)

Just as adding a quadratic term didn’t change the linearity of our model in Section 1.11.1, the model (4.1) is linear too. In the notation of Section 2.3.2), as long as we can write our model as

    mean D = A*β  (4.2) 

then by definition the model is linear. In the bike data model above, A would be


    A = |1 sin(2π·   1/365)|  (4.3)
        |1 sin(2π·   2/365)|
        |...               |
        |1 sin(2π·1095/365)|

But in this example, we have a known period, 365. In some other periodic setting, the period might be unknown, and would need to be estimated from our data. Our model might be, say,

    mean Y = β0 + β1*sin(2π·X/β2)  (4.4) 

where β2 is the unknown period. This does not correspond to (4.2). The model is still parametric, but is nonlinear.

Nonlinear parametric modeling, then, is the topic of this chapter. We’ll develop procedures for computing least squares estimates, and forming confidence intervals and p-values, again without assuming homoscedasticity. The bulk of the chapter will be devoted to the Generalized Linear Model (GLM), which is a widely-used broad class of nonlinear regression models. Two important special cases of the GLM will be the logistic model introduced briefly in Section 1.12.2, and Poisson regression.

## 4.1 Example: Enzyme Kinetics Model

Data for the famous Michaelis-Menten enzyme kinetics model is available in the nlstools package on CRAN. For the data set vmkm, we predict the reaction rate V from substrate concentration S. The model used was suggested by theoretical considerations to be

    E(V | S = t) = β1*t/(β2 + t)  (4.5) 

In the second data set, vmkmki [2], an additonal predictor I, inhibitor concentration, was added, with the model being

    E(V | S = t, I = u) = β1*t/(t + β2*(1 + u/β3))  (4.6) 

We’ll fit the model using R’s nls() function:

```{r comment=NA}
library(nlstools)
data(vmkmki)
regftn <- function(t, u, b1, b2, b3) b1*t/(t + b2*(1 + u/b3))
```

All nonlinear least-squares algorithms are iterative: We make an initial guess at the least-squares estimate, and from that, use the data to update the guess. Then we update the update, and so on, iterating until the guesses converge. In nls(), we specify the initial guess for the parameters, using the start argument, an R list [3]. Let’s set that up, and then run the analysis:

```{r comment=NA}
bstart <- list(b1 = 1, b2 = 1, b3 = 1)
```

The values 1 here were arbitrary, not informed guesses at all. Domain expertise can be helpful.

```{r comment=NA}
z <- nls(v ~ regftn(S, I, b1, b2, b3), data = vmkmki, start = list(b1 = 1, b2 = 1, b3 = 1))
z
```

So, β1 = 18.06 etc.

We can apply summary(), coef() and vcov() to the output of nls(), just as we did earlier with lm(). For example, here is the approximate covariance matrix of the coefficient vector:

```{r comment=NA}
vcov(z)
```

This assumes homoscedasticity. Under that assumption, an approximate 95% confidence interval for β1 would be

    18.06 ± 1.96*sqrt(0.4786776)  (4.7)

One can use the approach in Section 3.3.3 to adapt nls() to the heteroscedastic case, and we will do so in Section 4.2.2.

## 4.2 Least-Squares Computation

A point made in Section 1.3 was that the regression function, i.e. the conditional mean, is the optimal predictor function, minimizing mean squared prediction error. This still holds in the nonlinear (and even nonparametric) case. The problem is that in the nonlinear setting, the least-squares estimator does not have a nice, closed-form solution like (2.23) for the linear case. Let’s see how we can compute the solution through iterative approximation.

### 4.2.1 The Gauss-Newton Method

Denote the nonlinear model by

    E(Y | X = t) = g(t, β)  (4.8)

where both t and β are possibly vector-valued. In (4.5), for instance, t is a scalar but β is a vector. The least-squares estimate β^ is the value of b that minimizes

    sum([Y[i] − g(X[i], b)]^2, i = 1:n)  (4.9)

Many methods exist to minimize (4.9), most of which involve derivatives with respect to b. (The reason for the plural derivatives is that there is a partial derivative for each of the elements of b.)

The best intuitive explanation of derivative-based methods, which will also prove useful in a somewhat different context later in this chapter, is to set up a Taylor series approximation for g(Xi,b) (Appendix ??):

    g(X, b) ≈ g(X[i], β^) + h(X[i], β^)′*(b − β^)  (4.10)

where h(X[i], b) is the derivative vector of g(X[i], b) with respect to b, and the prime symbol, as usual, means matrix transpose (not a derivative). The


(4.13)                                           (2.12)
------------------------------------------------ ------
Y[i] − g(X[i], b[k−1]) + h(X[i], b[k−1])′*b[k−1] Y[i]
h(X[i], b[k−1])                                  X~[i]

**Table 4.1**

value of β^, is of course yet unknown, but let’s put that matter aside for now. Then (4.9) is approximately

    sum([Y[i] − g(X[i], β^) + h(X[i], β^)′*β^ − h(X[i], β^)′ b]^2, i = 1:n)  (4.11)

At iteration k we take our previous iteration b[k−1] to be an approximation to β^, and make that substitution in (4.11), yielding

    sum([Y[i] − g(X[i], b[k−1]) + h(X[i], b[k−1])′*b[k−1] − h(X[i], b[k−1])′*b]^2, i = 1:n)  (4.12)

Our b[k] is then the value that minimizes (4.12) over all possible values of b. But why is that minimization any easier than minimizing (4.9)? To see why, write (4.12) as

    sum([Y[i] − g(X[i], b[k−1]) + h(X[i], b[k−1])′*b[k−1] − h(X[i], b[k−1])′*b]^2, i = 1:n)  (4.13)

This should look familiar. It has exactly the same form as (2.12), with the correspondences shown in Table 4.1. In other words, what we have in (4.13) is a linear regression problem!

In other words, we can find the minimizing b in (4.13) using lm(). There is one small adjustment to be made, though. Recall that in (2.12), the quantity X~[i] includes a 1 term (Section 2.1), i.e. the first column of A in (2.13) consists of all ones. That is not the case in Table 4.1 (second row, first column), which we need to indicate in our lm() call. We can do this via specifying “-1” in the formula part of the call.

Another issue is the computation of h(). Instead of burdening the user with with this, it is typical to compute h() using numerical approximation, e.g. using R’s numericDeriv() function or the numDeriv package.

### 4.2.2 Eickert-White Asymptotic Standard Errors

As noted, nls() assumes homoscedasticity, which generally is a poor assumption (Section 2.4.2). It would be nice, then, to apply the Eickert-White method (Section 3.3.3). Actually, it is remarkably easy to adapt that method to the nonlinear computation above.

The key is to note the linear approximation (4.2.1). One way to look at this is that it has already set things up for us to use the delta method, which uses a linear approximation. Thus we can apply Eickert-White to the lm() output, say using vcovHC(), as in Section 3.3.4.

Below is code along these lines. It requires the user to run nlsLM(), an alternate version of nls() in the CRAN package minpack.lm [4],

```{r comment=NA}
library(minpack.lm) 
library(sandwich)

# uses output of nlsLM() of the minpack.lm package to get an # asymptotic covariance matrix without 
# assuming homoscedasticity

# arguments :
#   nlslmout: return value from nlsLM()
#
# value: approximate covariance matrix for the 
#        estimated parameter vector

nlsvcovhc <- function(nlslmout) {
  # notation: g(t,b) is the regression model,
  # where x is the vector of variables for a
  # given observation; b is the estimated parameter
  # vector; x is the matrix of predictor values
  b <- coef(nlslmout)
  m <- nlslmout$m
  # y − g:
  resid <- m$resid( )
  # row i of hmat will be deriv of g(x[i , ], b) 
  # with respect to b
  hmat <- m$gradient()
  # calculate the artificial ”x” and ”y” of 
  # the algorithm
  fakex <- hmat
  fakey <- resid + hmat %*% b
  # −1 means no constant term in the model
  lmout <- lm(fakey ~ fakex - 1)
  vcovHC(lmout)
}
```

In addition to nice convergence behavior, the advantage for us here of nlsLM() over nls() is that the former gives us access to the quantities we need in (4.13), especially the matrix of h() values. We then apply lm() one more time, to get an object of type ”lm”, needed by vcovHC().

Applying this to the enzyme data, we have

```{r comment=NA}
nlsvcovhc(z)
```

This is rather startling. Except for the estimated variance of β1, the estimated variances and covariances from Eickert-White are much larger than what nls() found under the assumption of homoscedasticity.

Of course, with only 60 observations, both of the estimated covariance matrices must be “taken with a grain of salt.” So, let’s compare the two approaches by performing a simulation. Here

    E(Y | X = t)= 1/(t′β*)  (4.14) 

where t = (t1, t2)′ and β = (β1, β2)′. We’ll take the components of X to be independent and exponentially distributed with mean 1.0, with the heteroscedasticity modeled as being such that the standard deviation of Y given X is proportional to the regression function value at X. We’ll use as a check the fact that a N(0, 1) variable is less than 1.28 90% of the time, Here is the simulation code:

```{r comment=NA}
sim <- function(n, nreps) { 
  b <- 1 : 2
  res <- replicate(nreps, {
    x <- matrix(rexp(2*n), ncol = 2)
    meany <- 1/(x %*% b)
    y <- meany + (runif(n) - 0.5)*meany 
    xy <- cbind(x, y)
    xy <- data.frame(xy)
    #nls.control(maxiter = 500) # added (ran in R, but not in Rmd?)
    nlout <- nls(X3 ~ 1/(b1*X1 + b2*X2),
      data = xy, start = list(b1 = 1, b2 = 1)) 
    b <- coef(nlout)
    vc <- vcov(nlout)
    vchc <- nlsvcovhc(nlout)
    z1 <- (b[1] - 1)/sqrt(vc[1, 1])
    z2 <- (b[1] - 1)/sqrt(vchc[1, 1]) 
    c(z1, z2)
  })
  print(mean(res[1, ] < 1.28)) 
  print(mean(res[2, ] < 1.28))
}
```

And here is a run of the code:

```{r comment=NA}
#sim(250, 2500) # runs in R, but not in Rmd - why?
```

That’s quite a difference! Eickert-White worked well, whereas assuming homoscedasticity fared quite poorly. (Similar results were obtained even for n = 100.)

### 4.2.3 Example: Bike Sharing Data

In our bike-sharing data (Section 1.12.2), there are two kinds of riders, registered and casual. We may be interested in factors determining the mix, i.e.

    registered/(registered + casual)  (4.15)

Since the mix proportion is between 0 and 1, we might try the logistic model, introduced in (4.18), in the context of classification, though the example here does not involve a classification problem, and use of glm() would be inappropriate. Here are the results:

```{r comment=NA}
# need to download "day.csv" and store in working directory
setwd("~/GitHub/R")
shar <- read.csv("day.csv", header = TRUE) 
shar$temp2 <- shar$temp^2
shar$summer <- as.integer(shar$season == 3)
names(shar)[15] <- "reg"
#shar$propreg <- shar$reg/(shar$reg + shar$cnt)
shar$propreg <- with(shar, reg/(reg + cnt))
#library(minpack.lm)
logit <- function(t1, t2, t3, t4, b0, b1, b2, b3, b4)
  1/(1 + exp(-b0 -b1*t1 -b2*t2 -b3*t3 -b4*t4)) 
z <- nlsLM(propreg ~
  logit(temp, temp2, workingday, summer, b0, b1, b2, b3, b4),
  data = shar, start = list(b0 = 1, b1 = 1, b2 = 1, b3 = 1, b4 = 1))
summary(z)
```

As expected, on working days, the proportion of registered riders is higher, as we are dealing with the commute crowd on those days. On the other hand, the proportion doesn’t seem to be much different during the summer, even though the vacationers would presumably add to the casual-rider count.

But are those standard errors trustworthy? Let’s look at the Eickert-White versions:

```{r comment=NA}
sqrt(diag(nlsvcovhc(z)))
```

Again, we see some substantial differences.

### 4.2.4 The "Elephant in the Room": Convergence Issues

So far we have sidestepped the fact that any iterative method runs the risk of non-convergence. Or it might converge to some point at which there is only a local minimum, not the global one — worse than non-convergence, in the sense that the user might be unaware of the situation.

For this reason, it is best to try multiple, diverse sets of starting values. In addition, there are refinements of the Gauss-Newton method that have better convergence behavior, such as the Levenberg-Marquardt method.

Gauss-Newton sometimes has a tendency to “overshoot,” producing too large an increment in b from one iteration to the next. Levenberg-Marquardt generates smaller increments. Interestingly it is a forerunner of ridge regression that we’ll discuss in Chapter 8. It is implemented in the CRAN package minpack.lm, which we used earlier in this chapter.

### 4.2.5 Example: Eckerle4 NIST Data

The U.S. National Institute of Standards and Technology has available several data sets that serve as test beds for nonlinear regression modeling. The one we’ll use here is called Eckerle4 [5]. The data are from a NIST study involving circular interference transmittance. Here we predict transmittance from wavelength, with a model like a normal density:

    mean transmittance = (β1/β2)*exp[−0.5((wavelength − β3)/β2)2]  (4.16) 

NIST also provides sets of starting values. For this data set, the two suggested starting vectors are (1, 10, 500) and (1.5, 5, 450), values that apparently came from informal inspection of a plot of the data, seen in Figure 4.1. It is clear, for instance, that β3 is around 450. The standard deviation of a normal curve is the distance from the center of the curve to either inflection point, say about 10 here. Since since the maximum value of the curve is about 0.4, we can then solve for an initial guess for β^1.

Figure 4.1: Eckerle4 Data

It turns out that ordinary nls() works for the second set but not the first:

```{r comment=NA}
# need to download "Eckerle4.dat" and store in working directory, edit,
# and save in CSV format
#eck <- read.table("Eckerle4.dat", header = TRUE)
eck <- read.csv("Eckerle4.csv", header = TRUE)
frm <- y ~ (b1/b2)*exp(-0.5*((x - b3)/b2)^2)
bstart
#nls(frm, data = eck, start = bstart)
#Error in nlsModel(formula, mf, start, wts) : 
#  singular gradient matrix at initial parameter estimates
bstart <- list(b1 = 1.5, b2 = 5, b3 = 450) 
nls(frm, data = eck, start = bstart)
```

But nlsLM() worked with both sets.

### 4.2.6 The Verdict

Nonlinear regression models can be powerful, but may be tricky to get to converge properly. Moreover, convergence might be achieved at a local, rather than global minimum, in which case the statistical outcome may be problematic. A thorough investigation of convergence (and fit) issues in any application is a must.

Fortunately, for the special case of Generalized Linear Models, the main focus of this chapter, convergence is rarely a problem. So, let’s start discussing GLM.

## 4.3 The Generalized Linear Model

Recall once again the logistic model, introduced in (4.18). We are dealing with a classification problem, so the Y takes on the values 0 and 1. Let X = (X(1), X(2), ..., X(p))′ denote the vector of our predictor variables.

### 4.3.1 Definition

Our model is

    P(Y = 1 | X(1) = t[1], ..., X(p)) = t[p]) =l(β0 + β1*t[1]+ ... + βp*t[p])  (4.17)

where

    l(s) = 1/(1 + exp(−s))  (4.18)

and t = (t[1], ..., t[p])′ [6].

The key point is that even though the right-hand side of (4.17) is not linear in t, it is a function of a linear expression in t, hence the term generalized linear model (GLM).

So, GLM is actually a broad class of models. We can use many different functions q() in place of l() in (4.18); for each such function, we have a different GLM.

### 4.3.2 Example: Poisson Regression

For example, setting q() = exp() gives us a model known as Poisson regression, which assumes

    E(Y = 1 | X(1) = t[1], …, X(p)) = t[p]) = exp(β0 + β1*t[1] + ... + βp*t[p])  (4.19)

In addition, GLM assumes a specified parametric class for the conditional distribution of Y given X , which we will denote F[Y|X]. In Poisson regression, this assumption is, not surprisingly, that the conditional distribution of Y given X is Poisson. In the logistic case, the assumption is trivially that the distribution is Bernoulli, i.e. binomial with number of trials equal to 1. Having such assumptions enables maximum likelihood estimation.

In particular, the core of GLM assumes that F[Y|X] belongs to an exponential family. This is formally defined as a parametric family whose probability density/mass function has the form

    exp[η(θ)*T(x) − A(θ) + B(x)]  (4.20)

where θ is the parameter vector and x is a value taken on by the random variable. Though this may seem imposing, it suffices to say that the above formulation includes many familiar distribution families such as Bernoulli, binomial, Poisson, exponential and normal. In the Poisson case, for in- stance, setting η(θ) = log(λ), T(x) = k, A(θ) = −λ and B(x) = −log(k!) yields the expression

    exp(−λ)*λ^k/k!  (4.21)

the famous form of the Poisson probability mass function.

GLM terminology centers around the link function, which is the functional inverse of our function q() above. For Poisson regression, the link function is the inverse of exp(), i.e. log(). For logit, set u = l(s) = 1/(1 + exp(−s)), and solve for s, giving us the link function,

    link(u) = u/(1 − u)  (4.22) 

### 4.3.3 GLM Computation

Though estimation in GLM uses maximum likelihood, it can be shown that the actual computation can be done extending the ideas in Section 4.2, i.e. via least-squares models. The only new aspect is the addition of a weight function, which works as follows.

Let’s review Section 3.11, which discussed weighted least squares in the case of a linear model. Using our usual notation μ(t) = E(Y | X = t) and σ^2(t) = Var(Y | X = t), the optimal estimator of β is the value of b that minimizes

    sum((Y[i] − X~[i]′b)^2/σ^2(X~[i]), i = 1:n)  (4.23)

Now consider the case of Poisson regression. One of the famous properties of the Poisson distribution family is that the variance equals the mean. Thus (4.9) becomes

    sum([Y[i] − g(X[i], b)]^2/g(X[i], b), i = 1:n)  (4.24)

Then (4.13) becomes

    sum([Y[i] − g(X[i], b[k−1]) + h(X[i], b[k−1])′*b[k−1] − h(X[i], b[k−1])′*b]^2/g(X[i], b[k−1]), i = 1:n)  (4.25)

and we again solve for the new iterate b[k] by calling lm(), this time making use of the latter’s weights argument.

We iterate as before, but now the weights are updated at each iteration too. For that reason, the process is known as iteratively re-weighted least squares.

### 4.3.4 R’s glm() Function

Of course, the glm() function does all this for us. For ordinary usage, the call is the same as for lm(), except for one extra argument, family. In the Poisson regression case, for example, the call looks like

    glm(y ~ x, family = poisson)

The family argument actually has its own online help entry:

```{r comment=NA}
?family
```

Ah, so the family argument is a function! There are built-in ones we can use, such as the poisson one we used above, or a user could define her own custom function.

Well, then, what are the arguments to such a function? A key argument is link, which is obviously the link function q() discussed above, which we found to be log() in the Poisson case.

For a logistic model, as noted earlier, F[Y|X] is binomial with number of trials m equal to 1. Recall that the variance of a binomial random variable with m trials is m\*r\*(1 − r), where r is the “success” probability on each trial.

Recall too that the mean of a 0-1-valued random variable is the probability of a 1. Putting all this together, we have

    σ^2(t) = μ(t)*[1 − μ(t)]  (4.26) 

Sure enough, this appears in the code of the built-in function binomial():

```{r comment=NA}
binomial
```

Let’s now turn to details of two of the most widely-used models, the logistic and Poisson.

## 4.4 GLM: the Logistic Model

The logistic regression model, introduced in Section 1.12.2, is by far the most popular nonlinear regression method. Here we are predicting a response variable Y that takes on the values 1 and 0, indicating which of two classes our unit belongs to. As we saw in Section 1.12.1, this indeed is a regression situation, as E(Y | X = t) reduces to P(Y = 1 | X = t).

The model, again, is

    P(Y = 1 | X = (t[1], ..., t[p])) = 1/(1 + exp(−(β0 + β1*t[1] + ... + βp*t[p]))  (4.27)

### 4.4.1 Motivation

We noted in Section 1.12.2 that the logistic model is appealing for two reasons: (a) It takes values in [0, 1], as a model for probabilities should, and (b) it is monotone in the predictor variables, as in the case of a linear model.

But there’s even more. It turns out that the logistic model is related to many familiar distribution families. We’ll show that in this section. In addition, the derivations here will deepen the reader’s insight into the various conditional probabilities involved in the overall classification problem.

To illustrate that, consider a very simple example of text classification, involving Twitter tweets. Suppose we wish to automatically classify tweets into those involving financial issues and all others. We’ll do that by having our code check whether a tweet contains words from a list of financial terms we’ve set up for this purpose, say bank, rate and so on.

Here Y is 1 or 0, for the financial and nonfinancial classes, and X is the number of occurrences of terms from the list. Suppose that from past data we know that among financial tweets, the number of occurrences of words from this list has a Poisson distribution with mean 1.8, while for nonfinancial tweets the mean is 0.2. Mathematically, that says that F[X|Y] = 1 is Poisson with mean 1.8, and F[X|Y] = 0 is Poisson with mean 0.2. (Be sure to distinguish the situation here, in which F[X|Y] is a Poisson distribution, from Poisson regression (Section 4.3.2), in which it is assumed that F[Y|X] is Poisson.) Finally, suppose 5% of all tweets are financial.

Recall once again (Section 1.12.1) that in the classification case, our regression function takes the form

    μ(t) = P(Y = 1 | X = t)  (4.28)

Let’s calculate this function:

    P(Y = 1 | X = t) 
    = P(Y = 1 and X = t)/P(X = t)  (4.29)
    = P(Y = 1 and X = t)/P(Y = 1 and X = t or Y = 1 and X = t)
    = π*P(X = t | Y = 1)/(π*P(X = t | Y = 1) + (1 − π)*P(X = t | Y = 0))

where π = P(Y = 1) is the population proportion of individuals in class 1. The numerator in (4.29) is

    0.05·exp(−1.8)*1.8^t/t!  (4.30)

and similarly the denominator is

    0.05·exp(−1.8)*1.8^t/t! + 0.95*exp(−0.2)*0.2^t/t!  (4.31)

Putting this into (4.29) and simplifying, we get

    P(Y = 1 | X = t) = 1/(1 + 19*exp(1.6*(1/9)^t))  (4.32) 
    = 1/(1 + exp(log(19) −0.2 −t*log(9))  (4.33)

That last expression is of the form

    1/(1 + exp[−(β0 + β1*t)])  (4.34)

with

    β0 = −log(19) + 0.2  (4.35)

and

    β1 = log(9)  (4.36)

In other words the setting in which F[X|Y] is Poisson implies the logistic model!

This is true too if F[X|Y] is an exponential distribution. Since this is a continuous distribution family rather than a discrete one, the quantities P (X = t | Y = i) in (4.32) must be replaced by density values:

    P(Y = 1 | X = t)
    = π*f[1](X = t | Y = 1)/(π*f[1](X = t | Y = 1) + (1 − π)*f[0](X = t | Y = 0))  (4.37)

where the within-class densities of X are

    f[i](w) = λ[i]*exp(−λ[i]*w), i = 0, 1  (4.38) 

After simplifying, we again obtain a logistic form.

Most important, consider the multivariate normal case: Say for groups i = 0, 1, F[X|Y] = i is a multivariate normal distribution with mean vector μi and covariance matrix Σ, where the latter does not have a subscript i. This is a generalization of the classical two-sample t-test setting, in which two (scalar) populations are assumed to have possibly different means but the same variance [7]. Again using (4.37), and going through a lot of algebra, we find that again P(Y = 1 | X = t) turns out to have a logistic form,

    P(Y = 1 | X = t) = 1/(1 + exp(−(β0 + β-′t)))  (4.39)

with

    β0 = log(1 − π) − log(π) + 0.5*(μ1′*μ1 − μ0′*μ0)  (4.40)

and

    β = (μ0 − μ1)′*inv(Σ)  (4.41)

where t is the vector of predictor variables, the β vector is broken down into (β0, β), and π is P(Y = 1). The messy form of the coefficients here is not important; instead, the point is that we find that the multivariate normal model implies the logistic model, giving the latter even more justification.

In summary:  Not only is the logistic model intuitively appealing because it is a monotonic function with values in (0, 1), but also because it is implied by various familiar parametric models for the within-class distribution of X.

No wonder the logit model is so popular!

### 4.4.2 Example: Pima Diabetes Data

Another famous UCI data set is from a study of the Pima tribe of Native Americans, involving factors associated with diabetes. There is data on 768 women [8]. Let’s predict diabetes from the other variables:

```{r comment=NA}
# need to download "pima-indians-diabetes.data" and store in working directory
pima <- read.csv("pima-indians-diabetes.data.txt", header = FALSE)
names(pima) <- c("NPreg", "Gluc", "BP", "Thick", "Insul", "BMI", "Genet", "Age", "Diab")
logitout <- glm(Diab ~ . , data = pima, family = binomial)
summary(logitout)
```

### 4.4.3 Interpretation of Coefficients

In nonlinear regression models, the parameters β[i] do not have the simple marginal interpretation they enjoy in the linear case. Statements like we made in Section 1.7.1.2, “We estimate that, on average, each extra year of age corresponds to almost a pound in extra weight,” are not possible here.

However, in the nonlinear case, the regression function is still defined as the conditional mean, or in the logit case, the conditional probability of a 1. Practical interpretation is definitely still possible, if slightly less convenient.

Consider for example the estimated Glucose coefficient in our diabetes data above, 0.035. Let’s apply that to the people similar to the first person in the data set:

```{r comment=NA}
pima[1, ]
```

Ignore the fact that this woman has diabetes. Let’s consider the population group of all women with the same characteristics as this one, i.e. all who have had 6 pregnancies, a glucose level of 148 and so on, through an age of 50. The estimated proportion of women in this population group is

    1/(1 + exp(−(8.4047 + 0.1232·6 + ... + 0.0149·50)))  (4.42)

We don’t have to plug these numbers in by hand, of course:

```{r comment=NA}
l <- function(t) 1/(1 + exp(-t))
pima1 <- pima[1, -9] # exclude diab. var.
pima1 <- unlist(pima[1, -9]) # had been a data frame 
l(coef(logitout) %*% c(1, pima1))
```

So, about 72% of women in this population group have diabetes. But what about the population group of the same characteristics, but of age 40 instead of 50?

```{r comment=NA}
w <- pima1
w["Age"] <- 40
l(coef(logitout) %*% c(1, w))
```

Only about 69% of the younger women have diabetes.

So, there is an effect of age on developing diabetes, but only a mild one; a 10-year increase in age only increased the chance of diabetes by about 3.1%. However, note carefully that this was for women having a given set of the other factors, e.g. 6 pregnancies. Let’s look at a different population group, those with 2 pregnancies and a glucose level of 120, comparing 40- and 50-year-olds:

```{r comment=NA}
u <- pima1 
u[1] <- 2
u[2] <- 100
v <- u
v[8] <- 40
l(coef(logitout) %*% c(1, u))
l(coef(logitout) %*% c(1, v))
```

So here, the 10-year age effect was somewhat less, about 2.5%. A more careful analysis would involve calculating standard errors for these numbers, but the chief point here is that the effect of a factor in nonlinear situations depends on the values of the other factors.

    P(Y = 0 | X(1) = t[1], ..., X(p)) = t[p]) = 1 − l(β0 + β1*t[1] + ... + βp*t[p])  (4.43) 

Some analysts like to look at the log-odds ratio,

    P(Y = 1 | X(1) = t[1], ..., X(p)) = t[p])/P(Y = 0 | X(1) = t[1], ... , X(p)) = t[p])  (4.44)

in this case the ratio of the probability of having and not having the disease. By Equation (4.17), this simplifies to

    β0 + β1*t[1] + ... + βp*t[p]  (4.45)

— a linear function! Thus, in interpreting the coefficients output from a logistic analysis, it is convenient to look at the log-odds ratio, as it gives us a single number for each factor. This may be sufficient for the application at hand, but a more thorough analysis should consider the effects of the factors on the probabilities themselves.

### 4.4.4 The predict() Function

In the previous section, we evaluated the estimated regression function (and thus predicted values as well) the straightforward but messy way, e.g.

```{r comment=NA}
l(coef(logitout) %*% c(1, v))
```

The easy way is to use R’s predict() function:

```{r comment=NA}
predict(object = logitout, newdata = pima[1, -9], type = "response")
```

Let’s see what just happened. First, predict() is what is called a generic function in R. What this means is that it is not a single function, but rather an umbrella leading into a broad family of functions, depending on the class of the object on which it is invoked.

In our case here, we invoked it on logitout. What is the class of that object?

```{r comment=NA}
class(logitout)
```

So, it is an object of class ”glm” (and, we see, the latter is a subclass of the class ”lm”).

In processing our call to predict(), the R interpreter found that our object had class ”glm”, and thus looked for a function predict.glm(), which the authors of glm() had written for us. So, in R terminology, the interpreter dispatched, i.e. relayed, our priedct()call to predict.glm(). What did the latter then do?

The argument newdata is a data frame consisting of what its name implies — new cases to predict. As you will recall, the way we predict a new case in regression analysis is to take as our prediction the value of the regression function for that case. The net result is then that predict() is giving us the value of the regression function at our requested points, in this case for the population group of all women with the same traits as the first person in our data.

So, predict() while doesn’t give us any new capabilities, it certainly makes things more convenient. The reader should ponder, for instance, how to use this function to simplify the code in Section 1.9.4.1.

### 4.4.5 Linear Boundary

In (4.27), which values of t = (t[1], ..., t[p])′ will cause us to guess Y = 1 and which will result in a guess of Y = 0? The boundary occurs when (4.27) has the value 0.5. In other words, the boundary consists of all t such that 

    β0 + β1*t1 + ... + βp*t[p] = 0  (4.46)

So, the boundary has linear form, a hyperplane in p-dimensional space. This may seem somewhat abstract now, but it will have value later on.

## 4.5 GLM: the Poisson Model

Since in the above data the number of pregnancies is a count, we might consider predicting it using Poisson regression. (The reader may wonder why we may be interested in such a “reverse” prediction. It could occur, for instance, with multiple imputation methods to deal with missing data.) Here’s how we can do this with glm():

```{r comment=NA}
poisout <- glm(NPreg ~ . , data = pima, family = poisson)
summary(poisout)
```

On the other hand, even if we believe that our count data follow a Poisson distribution, there is no law dictating that we use Poisson regression, i.e. the model (4.3.2). The main motivation for using exp() in that model is to ensure that our regression function is nonnegative, conforming to the non-negative nature of Poisson random variables. This is not unreasonable, but as noted in a somewhat different context in Section 3.3.7, transformations like this can produce distortions. Let’s try an alternative:

```{r comment=NA}
quasiout <- glm(NPreg ~ ., data = pima, family = quasi(variance = "mu^2"), start = rep(1 , 9))
```

This “quasi” family is a catch-all option, specifying a linear model but here allowing us to specify a Poisson variance function.

Well, then, which model performed better? As a rough, quick look, ignoring issues of overfitting and the like, let’s consider R2. This quantity is not calculated by glm(), but recall from Section 2.7.2 that R2 is the squared correlation between the predicted and actual Y values. This quantity makes sense for any regression situation, so let’s calculate it here:

```{r comment=NA}
cor(poisout$fitted.values, poisout$y)^2
cor(quasiout$fitted.values, quasiout$y)^2
```

The “unorthodox” model performed better. We cannot generalize from this, but it does show again that one must use transformations carefully.

[1] We’ll ignore the issue of leap years here, to keep things simple.

[2] There were 72 observation in this data, but the last 12 appear to be anomalous (gradient 0 in all elements), and thus were excluded.

[3] This also gives the code a chance to learn the names of the parameters, needed for computation of derivatives.

[4] This version is needed here because it provides the intermediate quantities we need from the computation. However, we will see in Section 4.2.4 that this version has other important advantages as well.

[5] See http://www.itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Eckerle4.dat.

[6] Recall from Section 1.12.1 that te classification problem is a special case of regression.

[7] It is also the setting for *Fisher’s Linear Discriminant Analysis*, to be discussed in Section ??.

[8] The data set is at https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes. I have added a header record to the file.
